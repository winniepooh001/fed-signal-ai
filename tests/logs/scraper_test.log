2025-06-16 22:37:25 | INFO     | logging_config.py:93   | initialize() | ============================================================
2025-06-16 22:37:25 | INFO     | logging_config.py:94   | initialize() | UNIFIED SCREENER SYSTEM STARTING
2025-06-16 22:37:25 | INFO     | logging_config.py:95   | initialize() | Log Level: INFO
2025-06-16 22:37:25 | INFO     | logging_config.py:96   | initialize() | Log File: logs\scraper_test.log
2025-06-16 22:37:25 | INFO     | logging_config.py:97   | initialize() | Console Output: True
2025-06-16 22:37:25 | INFO     | logging_config.py:98   | initialize() | All modules will log to this unified logger
2025-06-16 22:37:25 | INFO     | logging_config.py:99   | initialize() | ============================================================
2025-06-16 22:37:25 | INFO     | test_scraper.py:324  | main() | üß™ STARTING MULTI-SCRAPER SYSTEM TESTS
2025-06-16 22:37:25 | INFO     | test_scraper.py:325  | main() | ================================================================================
2025-06-16 22:37:25 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Fed Scraper test...
2025-06-16 22:37:25 | INFO     | test_scraper.py:29   | test_fed_scraper() | ============================================================
2025-06-16 22:37:25 | INFO     | test_scraper.py:30   | test_fed_scraper() | TESTING FEDERAL RESERVE SCRAPER
2025-06-16 22:37:25 | INFO     | test_scraper.py:31   | test_fed_scraper() | ============================================================
2025-06-16 22:37:26 | ERROR    | test_scraper.py:66   | test_fed_scraper() | ‚ùå Fed scraper test crashed: Table 'scraped_data' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 35, in test_fed_scraper
    from database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\__init__.py", line 1, in <module>
    from database.database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\database.py", line 9, in <module>
    from database.models import Base, ScrapedData, ScreenerInput, ScreenerResult, AgentExecution, DataEmbedding, LLMUsage
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\models.py", line 140, in <module>
    class ScrapedData(Base):
    ...<24 lines>...
        )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_api.py", line 198, in __init__
    _as_declarative(reg, cls, dict_)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 244, in _as_declarative
    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 325, in setup_mapping
    return _ClassScanMapperConfig(
        registry, cls_, dict_, table, mapper_kw
    )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 576, in __init__
    self._setup_table(table)
    ~~~~~~~~~~~~~~~~~^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 1755, in _setup_table
    table_cls(
    ~~~~~~~~~^
        tablename,
        ^^^^^^^^^^
    ...<3 lines>...
        **table_kw,
        ^^^^^^^^^^^
    ),
    ^
  File "<string>", line 2, in __new__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\util\deprecations.py", line 281, in warned
    return fn(*args, **kwargs)  # type: ignore[no-any-return]
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\sql\schema.py", line 429, in __new__
    return cls._new(*args, **kw)
           ~~~~~~~~^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\sql\schema.py", line 461, in _new
    raise exc.InvalidRequestError(
    ...<5 lines>...
    )
sqlalchemy.exc.InvalidRequestError: Table 'scraped_data' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
2025-06-16 22:37:26 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:37:26 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Reddit WSB Scraper test...
2025-06-16 22:37:26 | INFO     | test_scraper.py:72   | test_reddit_scraper() | ============================================================
2025-06-16 22:37:26 | INFO     | test_scraper.py:73   | test_reddit_scraper() | TESTING REDDIT WSB SCRAPER
2025-06-16 22:37:26 | INFO     | test_scraper.py:74   | test_reddit_scraper() | ============================================================
2025-06-16 22:37:26 | ERROR    | test_scraper.py:118  | test_reddit_scraper() | ‚ùå Reddit WSB scraper test crashed: Table 'scraped_data' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 78, in test_reddit_scraper
    from database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\__init__.py", line 1, in <module>
    from database.database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\database.py", line 9, in <module>
    from database.models import Base, ScrapedData, ScreenerInput, ScreenerResult, AgentExecution, DataEmbedding, LLMUsage
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\models.py", line 140, in <module>
    class ScrapedData(Base):
    ...<24 lines>...
        )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_api.py", line 198, in __init__
    _as_declarative(reg, cls, dict_)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 244, in _as_declarative
    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 325, in setup_mapping
    return _ClassScanMapperConfig(
        registry, cls_, dict_, table, mapper_kw
    )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 576, in __init__
    self._setup_table(table)
    ~~~~~~~~~~~~~~~~~^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 1755, in _setup_table
    table_cls(
    ~~~~~~~~~^
        tablename,
        ^^^^^^^^^^
    ...<3 lines>...
        **table_kw,
        ^^^^^^^^^^^
    ),
    ^
  File "<string>", line 2, in __new__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\util\deprecations.py", line 281, in warned
    return fn(*args, **kwargs)  # type: ignore[no-any-return]
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\sql\schema.py", line 429, in __new__
    return cls._new(*args, **kw)
           ~~~~~~~~^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\sql\schema.py", line 461, in _new
    raise exc.InvalidRequestError(
    ...<5 lines>...
    )
sqlalchemy.exc.InvalidRequestError: Table 'scraped_data' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
2025-06-16 22:37:26 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:37:26 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Scraper Manager test...
2025-06-16 22:37:26 | INFO     | test_scraper.py:124  | test_scraper_manager() | ============================================================
2025-06-16 22:37:26 | INFO     | test_scraper.py:125  | test_scraper_manager() | TESTING SCRAPER MANAGER
2025-06-16 22:37:26 | INFO     | test_scraper.py:126  | test_scraper_manager() | ============================================================
2025-06-16 22:37:26 | ERROR    | test_scraper.py:194  | test_scraper_manager() | ‚ùå Scraper manager test crashed: Table 'scraped_data' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 129, in test_scraper_manager
    from scrapers.scraper_manager import ScraperManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\scraper_manager.py", line 11, in <module>
    from database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\__init__.py", line 1, in <module>
    from database.database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\database.py", line 9, in <module>
    from database.models import Base, ScrapedData, ScreenerInput, ScreenerResult, AgentExecution, DataEmbedding, LLMUsage
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\models.py", line 140, in <module>
    class ScrapedData(Base):
    ...<24 lines>...
        )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_api.py", line 198, in __init__
    _as_declarative(reg, cls, dict_)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 244, in _as_declarative
    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 325, in setup_mapping
    return _ClassScanMapperConfig(
        registry, cls_, dict_, table, mapper_kw
    )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 576, in __init__
    self._setup_table(table)
    ~~~~~~~~~~~~~~~~~^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 1755, in _setup_table
    table_cls(
    ~~~~~~~~~^
        tablename,
        ^^^^^^^^^^
    ...<3 lines>...
        **table_kw,
        ^^^^^^^^^^^
    ),
    ^
  File "<string>", line 2, in __new__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\util\deprecations.py", line 281, in warned
    return fn(*args, **kwargs)  # type: ignore[no-any-return]
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\sql\schema.py", line 429, in __new__
    return cls._new(*args, **kw)
           ~~~~~~~~^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\sql\schema.py", line 461, in _new
    raise exc.InvalidRequestError(
    ...<5 lines>...
    )
sqlalchemy.exc.InvalidRequestError: Table 'scraped_data' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
2025-06-16 22:37:26 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:37:26 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Change Detection test...
2025-06-16 22:37:26 | INFO     | test_scraper.py:200  | test_change_detection() | ============================================================
2025-06-16 22:37:26 | INFO     | test_scraper.py:201  | test_change_detection() | TESTING CHANGE DETECTION
2025-06-16 22:37:26 | INFO     | test_scraper.py:202  | test_change_detection() | ============================================================
2025-06-16 22:37:26 | ERROR    | test_scraper.py:244  | test_change_detection() | ‚ùå Change detection test crashed: Table 'scraped_data' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 205, in test_change_detection
    from scrapers.scraper_manager import ScraperManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\scraper_manager.py", line 11, in <module>
    from database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\__init__.py", line 1, in <module>
    from database.database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\database.py", line 9, in <module>
    from database.models import Base, ScrapedData, ScreenerInput, ScreenerResult, AgentExecution, DataEmbedding, LLMUsage
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\models.py", line 140, in <module>
    class ScrapedData(Base):
    ...<24 lines>...
        )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_api.py", line 198, in __init__
    _as_declarative(reg, cls, dict_)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 244, in _as_declarative
    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 325, in setup_mapping
    return _ClassScanMapperConfig(
        registry, cls_, dict_, table, mapper_kw
    )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 576, in __init__
    self._setup_table(table)
    ~~~~~~~~~~~~~~~~~^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 1755, in _setup_table
    table_cls(
    ~~~~~~~~~^
        tablename,
        ^^^^^^^^^^
    ...<3 lines>...
        **table_kw,
        ^^^^^^^^^^^
    ),
    ^
  File "<string>", line 2, in __new__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\util\deprecations.py", line 281, in warned
    return fn(*args, **kwargs)  # type: ignore[no-any-return]
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\sql\schema.py", line 429, in __new__
    return cls._new(*args, **kw)
           ~~~~~~~~^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\sql\schema.py", line 461, in _new
    raise exc.InvalidRequestError(
    ...<5 lines>...
    )
sqlalchemy.exc.InvalidRequestError: Table 'scraped_data' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
2025-06-16 22:37:26 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:37:26 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Database Cleanup test...
2025-06-16 22:37:26 | INFO     | test_scraper.py:250  | test_database_cleanup() | ============================================================
2025-06-16 22:37:26 | INFO     | test_scraper.py:251  | test_database_cleanup() | TESTING DATABASE CLEANUP
2025-06-16 22:37:26 | INFO     | test_scraper.py:252  | test_database_cleanup() | ============================================================
2025-06-16 22:37:26 | ERROR    | test_scraper.py:276  | test_database_cleanup() | ‚ùå Database cleanup test crashed: Table 'scraped_data' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 255, in test_database_cleanup
    from scrapers.scraper_manager import ScraperManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\scraper_manager.py", line 11, in <module>
    from database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\__init__.py", line 1, in <module>
    from database.database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\database.py", line 9, in <module>
    from database.models import Base, ScrapedData, ScreenerInput, ScreenerResult, AgentExecution, DataEmbedding, LLMUsage
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\models.py", line 140, in <module>
    class ScrapedData(Base):
    ...<24 lines>...
        )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_api.py", line 198, in __init__
    _as_declarative(reg, cls, dict_)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 244, in _as_declarative
    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 325, in setup_mapping
    return _ClassScanMapperConfig(
        registry, cls_, dict_, table, mapper_kw
    )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 576, in __init__
    self._setup_table(table)
    ~~~~~~~~~~~~~~~~~^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 1755, in _setup_table
    table_cls(
    ~~~~~~~~~^
        tablename,
        ^^^^^^^^^^
    ...<3 lines>...
        **table_kw,
        ^^^^^^^^^^^
    ),
    ^
  File "<string>", line 2, in __new__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\util\deprecations.py", line 281, in warned
    return fn(*args, **kwargs)  # type: ignore[no-any-return]
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\sql\schema.py", line 429, in __new__
    return cls._new(*args, **kw)
           ~~~~~~~~^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\sql\schema.py", line 461, in _new
    raise exc.InvalidRequestError(
    ...<5 lines>...
    )
sqlalchemy.exc.InvalidRequestError: Table 'scraped_data' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
2025-06-16 22:37:26 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:37:26 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Performance Test test...
2025-06-16 22:37:26 | INFO     | test_scraper.py:282  | run_performance_test() | ============================================================
2025-06-16 22:37:26 | INFO     | test_scraper.py:283  | run_performance_test() | RUNNING PERFORMANCE TEST
2025-06-16 22:37:26 | INFO     | test_scraper.py:284  | run_performance_test() | ============================================================
2025-06-16 22:37:26 | ERROR    | test_scraper.py:317  | run_performance_test() | ‚ùå Performance test crashed: Table 'scraped_data' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 287, in run_performance_test
    from scrapers.scraper_manager import ScraperManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\scraper_manager.py", line 11, in <module>
    from database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\__init__.py", line 1, in <module>
    from database.database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\database.py", line 9, in <module>
    from database.models import Base, ScrapedData, ScreenerInput, ScreenerResult, AgentExecution, DataEmbedding, LLMUsage
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\models.py", line 140, in <module>
    class ScrapedData(Base):
    ...<24 lines>...
        )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_api.py", line 198, in __init__
    _as_declarative(reg, cls, dict_)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 244, in _as_declarative
    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 325, in setup_mapping
    return _ClassScanMapperConfig(
        registry, cls_, dict_, table, mapper_kw
    )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 576, in __init__
    self._setup_table(table)
    ~~~~~~~~~~~~~~~~~^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 1755, in _setup_table
    table_cls(
    ~~~~~~~~~^
        tablename,
        ^^^^^^^^^^
    ...<3 lines>...
        **table_kw,
        ^^^^^^^^^^^
    ),
    ^
  File "<string>", line 2, in __new__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\util\deprecations.py", line 281, in warned
    return fn(*args, **kwargs)  # type: ignore[no-any-return]
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\sql\schema.py", line 429, in __new__
    return cls._new(*args, **kw)
           ~~~~~~~~^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\sql\schema.py", line 461, in _new
    raise exc.InvalidRequestError(
    ...<5 lines>...
    )
sqlalchemy.exc.InvalidRequestError: Table 'scraped_data' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
2025-06-16 22:37:26 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:37:26 | INFO     | test_scraper.py:350  | main() | 
================================================================================
2025-06-16 22:37:26 | INFO     | test_scraper.py:351  | main() | SCRAPER SYSTEM TEST SUMMARY
2025-06-16 22:37:26 | INFO     | test_scraper.py:352  | main() | ================================================================================
2025-06-16 22:37:26 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Fed Scraper
2025-06-16 22:37:26 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Reddit WSB Scraper
2025-06-16 22:37:26 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Scraper Manager
2025-06-16 22:37:26 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Change Detection
2025-06-16 22:37:26 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Database Cleanup
2025-06-16 22:37:26 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Performance Test
2025-06-16 22:37:26 | INFO     | test_scraper.py:361  | main() | --------------------------------------------------------------------------------
2025-06-16 22:37:26 | INFO     | test_scraper.py:362  | main() | OVERALL RESULT: 0/6 tests passed
2025-06-16 22:37:26 | ERROR    | test_scraper.py:368  | main() | ‚ö†Ô∏è  6 tests failed. Check the logs above for details.
2025-06-16 22:41:15 | INFO     | logging_config.py:93   | initialize() | ============================================================
2025-06-16 22:41:15 | INFO     | logging_config.py:94   | initialize() | UNIFIED SCREENER SYSTEM STARTING
2025-06-16 22:41:15 | INFO     | logging_config.py:95   | initialize() | Log Level: INFO
2025-06-16 22:41:15 | INFO     | logging_config.py:96   | initialize() | Log File: logs\scraper_test.log
2025-06-16 22:41:15 | INFO     | logging_config.py:97   | initialize() | Console Output: True
2025-06-16 22:41:15 | INFO     | logging_config.py:98   | initialize() | All modules will log to this unified logger
2025-06-16 22:41:15 | INFO     | logging_config.py:99   | initialize() | ============================================================
2025-06-16 22:41:15 | INFO     | test_scraper.py:324  | main() | üß™ STARTING MULTI-SCRAPER SYSTEM TESTS
2025-06-16 22:41:15 | INFO     | test_scraper.py:325  | main() | ================================================================================
2025-06-16 22:41:15 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Fed Scraper test...
2025-06-16 22:41:15 | INFO     | test_scraper.py:29   | test_fed_scraper() | ============================================================
2025-06-16 22:41:15 | INFO     | test_scraper.py:30   | test_fed_scraper() | TESTING FEDERAL RESERVE SCRAPER
2025-06-16 22:41:15 | INFO     | test_scraper.py:31   | test_fed_scraper() | ============================================================
2025-06-16 22:41:15 | ERROR    | test_scraper.py:66   | test_fed_scraper() | ‚ùå Fed scraper test crashed: Attribute name 'metadata' is reserved when using the Declarative API.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 35, in test_fed_scraper
    from database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\__init__.py", line 1, in <module>
    from database.database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\database.py", line 9, in <module>
    from database.models import Base, ScrapedData, ScreenerInput, ScreenerResult, AgentExecution, DataEmbedding, LLMUsage
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\models.py", line 186, in <module>
    class TrendingTicker(Base):
    ...<16 lines>...
        )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_api.py", line 198, in __init__
    _as_declarative(reg, cls, dict_)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 244, in _as_declarative
    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 325, in setup_mapping
    return _ClassScanMapperConfig(
        registry, cls_, dict_, table, mapper_kw
    )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 572, in __init__
    self._extract_mappable_attributes()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 1529, in _extract_mappable_attributes
    raise exc.InvalidRequestError(
    ...<2 lines>...
    )
sqlalchemy.exc.InvalidRequestError: Attribute name 'metadata' is reserved when using the Declarative API.
2025-06-16 22:41:15 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:41:15 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Reddit WSB Scraper test...
2025-06-16 22:41:15 | INFO     | test_scraper.py:72   | test_reddit_scraper() | ============================================================
2025-06-16 22:41:15 | INFO     | test_scraper.py:73   | test_reddit_scraper() | TESTING REDDIT WSB SCRAPER
2025-06-16 22:41:15 | INFO     | test_scraper.py:74   | test_reddit_scraper() | ============================================================
2025-06-16 22:41:15 | ERROR    | test_scraper.py:118  | test_reddit_scraper() | ‚ùå Reddit WSB scraper test crashed: Attribute name 'metadata' is reserved when using the Declarative API.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 78, in test_reddit_scraper
    from database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\__init__.py", line 1, in <module>
    from database.database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\database.py", line 9, in <module>
    from database.models import Base, ScrapedData, ScreenerInput, ScreenerResult, AgentExecution, DataEmbedding, LLMUsage
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\models.py", line 186, in <module>
    class TrendingTicker(Base):
    ...<16 lines>...
        )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_api.py", line 198, in __init__
    _as_declarative(reg, cls, dict_)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 244, in _as_declarative
    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 325, in setup_mapping
    return _ClassScanMapperConfig(
        registry, cls_, dict_, table, mapper_kw
    )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 572, in __init__
    self._extract_mappable_attributes()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 1529, in _extract_mappable_attributes
    raise exc.InvalidRequestError(
    ...<2 lines>...
    )
sqlalchemy.exc.InvalidRequestError: Attribute name 'metadata' is reserved when using the Declarative API.
2025-06-16 22:41:15 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:41:15 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Scraper Manager test...
2025-06-16 22:41:15 | INFO     | test_scraper.py:124  | test_scraper_manager() | ============================================================
2025-06-16 22:41:15 | INFO     | test_scraper.py:125  | test_scraper_manager() | TESTING SCRAPER MANAGER
2025-06-16 22:41:15 | INFO     | test_scraper.py:126  | test_scraper_manager() | ============================================================
2025-06-16 22:41:15 | ERROR    | test_scraper.py:194  | test_scraper_manager() | ‚ùå Scraper manager test crashed: Attribute name 'metadata' is reserved when using the Declarative API.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 129, in test_scraper_manager
    from scrapers.scraper_manager import ScraperManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\scraper_manager.py", line 11, in <module>
    from database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\__init__.py", line 1, in <module>
    from database.database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\database.py", line 9, in <module>
    from database.models import Base, ScrapedData, ScreenerInput, ScreenerResult, AgentExecution, DataEmbedding, LLMUsage
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\models.py", line 186, in <module>
    class TrendingTicker(Base):
    ...<16 lines>...
        )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_api.py", line 198, in __init__
    _as_declarative(reg, cls, dict_)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 244, in _as_declarative
    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 325, in setup_mapping
    return _ClassScanMapperConfig(
        registry, cls_, dict_, table, mapper_kw
    )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 572, in __init__
    self._extract_mappable_attributes()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 1529, in _extract_mappable_attributes
    raise exc.InvalidRequestError(
    ...<2 lines>...
    )
sqlalchemy.exc.InvalidRequestError: Attribute name 'metadata' is reserved when using the Declarative API.
2025-06-16 22:41:15 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:41:15 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Change Detection test...
2025-06-16 22:41:15 | INFO     | test_scraper.py:200  | test_change_detection() | ============================================================
2025-06-16 22:41:15 | INFO     | test_scraper.py:201  | test_change_detection() | TESTING CHANGE DETECTION
2025-06-16 22:41:15 | INFO     | test_scraper.py:202  | test_change_detection() | ============================================================
2025-06-16 22:41:15 | ERROR    | test_scraper.py:244  | test_change_detection() | ‚ùå Change detection test crashed: Attribute name 'metadata' is reserved when using the Declarative API.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 205, in test_change_detection
    from scrapers.scraper_manager import ScraperManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\scraper_manager.py", line 11, in <module>
    from database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\__init__.py", line 1, in <module>
    from database.database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\database.py", line 9, in <module>
    from database.models import Base, ScrapedData, ScreenerInput, ScreenerResult, AgentExecution, DataEmbedding, LLMUsage
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\models.py", line 186, in <module>
    class TrendingTicker(Base):
    ...<16 lines>...
        )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_api.py", line 198, in __init__
    _as_declarative(reg, cls, dict_)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 244, in _as_declarative
    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 325, in setup_mapping
    return _ClassScanMapperConfig(
        registry, cls_, dict_, table, mapper_kw
    )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 572, in __init__
    self._extract_mappable_attributes()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 1529, in _extract_mappable_attributes
    raise exc.InvalidRequestError(
    ...<2 lines>...
    )
sqlalchemy.exc.InvalidRequestError: Attribute name 'metadata' is reserved when using the Declarative API.
2025-06-16 22:41:15 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:41:15 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Database Cleanup test...
2025-06-16 22:41:15 | INFO     | test_scraper.py:250  | test_database_cleanup() | ============================================================
2025-06-16 22:41:15 | INFO     | test_scraper.py:251  | test_database_cleanup() | TESTING DATABASE CLEANUP
2025-06-16 22:41:15 | INFO     | test_scraper.py:252  | test_database_cleanup() | ============================================================
2025-06-16 22:41:15 | ERROR    | test_scraper.py:276  | test_database_cleanup() | ‚ùå Database cleanup test crashed: Attribute name 'metadata' is reserved when using the Declarative API.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 255, in test_database_cleanup
    from scrapers.scraper_manager import ScraperManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\scraper_manager.py", line 11, in <module>
    from database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\__init__.py", line 1, in <module>
    from database.database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\database.py", line 9, in <module>
    from database.models import Base, ScrapedData, ScreenerInput, ScreenerResult, AgentExecution, DataEmbedding, LLMUsage
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\models.py", line 186, in <module>
    class TrendingTicker(Base):
    ...<16 lines>...
        )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_api.py", line 198, in __init__
    _as_declarative(reg, cls, dict_)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 244, in _as_declarative
    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 325, in setup_mapping
    return _ClassScanMapperConfig(
        registry, cls_, dict_, table, mapper_kw
    )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 572, in __init__
    self._extract_mappable_attributes()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 1529, in _extract_mappable_attributes
    raise exc.InvalidRequestError(
    ...<2 lines>...
    )
sqlalchemy.exc.InvalidRequestError: Attribute name 'metadata' is reserved when using the Declarative API.
2025-06-16 22:41:15 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:41:15 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Performance Test test...
2025-06-16 22:41:15 | INFO     | test_scraper.py:282  | run_performance_test() | ============================================================
2025-06-16 22:41:15 | INFO     | test_scraper.py:283  | run_performance_test() | RUNNING PERFORMANCE TEST
2025-06-16 22:41:15 | INFO     | test_scraper.py:284  | run_performance_test() | ============================================================
2025-06-16 22:41:15 | ERROR    | test_scraper.py:317  | run_performance_test() | ‚ùå Performance test crashed: Attribute name 'metadata' is reserved when using the Declarative API.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 287, in run_performance_test
    from scrapers.scraper_manager import ScraperManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\scraper_manager.py", line 11, in <module>
    from database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\__init__.py", line 1, in <module>
    from database.database import DatabaseManager
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\database.py", line 9, in <module>
    from database.models import Base, ScrapedData, ScreenerInput, ScreenerResult, AgentExecution, DataEmbedding, LLMUsage
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\database\models.py", line 186, in <module>
    class TrendingTicker(Base):
    ...<16 lines>...
        )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_api.py", line 198, in __init__
    _as_declarative(reg, cls, dict_)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 244, in _as_declarative
    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 325, in setup_mapping
    return _ClassScanMapperConfig(
        registry, cls_, dict_, table, mapper_kw
    )
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 572, in __init__
    self._extract_mappable_attributes()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 1529, in _extract_mappable_attributes
    raise exc.InvalidRequestError(
    ...<2 lines>...
    )
sqlalchemy.exc.InvalidRequestError: Attribute name 'metadata' is reserved when using the Declarative API.
2025-06-16 22:41:15 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:41:15 | INFO     | test_scraper.py:350  | main() | 
================================================================================
2025-06-16 22:41:15 | INFO     | test_scraper.py:351  | main() | SCRAPER SYSTEM TEST SUMMARY
2025-06-16 22:41:15 | INFO     | test_scraper.py:352  | main() | ================================================================================
2025-06-16 22:41:15 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Fed Scraper
2025-06-16 22:41:15 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Reddit WSB Scraper
2025-06-16 22:41:15 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Scraper Manager
2025-06-16 22:41:15 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Change Detection
2025-06-16 22:41:15 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Database Cleanup
2025-06-16 22:41:15 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Performance Test
2025-06-16 22:41:15 | INFO     | test_scraper.py:361  | main() | --------------------------------------------------------------------------------
2025-06-16 22:41:15 | INFO     | test_scraper.py:362  | main() | OVERALL RESULT: 0/6 tests passed
2025-06-16 22:41:15 | ERROR    | test_scraper.py:368  | main() | ‚ö†Ô∏è  6 tests failed. Check the logs above for details.
2025-06-16 22:41:39 | INFO     | logging_config.py:93   | initialize() | ============================================================
2025-06-16 22:41:39 | INFO     | logging_config.py:94   | initialize() | UNIFIED SCREENER SYSTEM STARTING
2025-06-16 22:41:39 | INFO     | logging_config.py:95   | initialize() | Log Level: INFO
2025-06-16 22:41:39 | INFO     | logging_config.py:96   | initialize() | Log File: logs\scraper_test.log
2025-06-16 22:41:39 | INFO     | logging_config.py:97   | initialize() | Console Output: True
2025-06-16 22:41:39 | INFO     | logging_config.py:98   | initialize() | All modules will log to this unified logger
2025-06-16 22:41:39 | INFO     | logging_config.py:99   | initialize() | ============================================================
2025-06-16 22:41:39 | INFO     | test_scraper.py:324  | main() | üß™ STARTING MULTI-SCRAPER SYSTEM TESTS
2025-06-16 22:41:39 | INFO     | test_scraper.py:325  | main() | ================================================================================
2025-06-16 22:41:39 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Fed Scraper test...
2025-06-16 22:41:39 | INFO     | test_scraper.py:29   | test_fed_scraper() | ============================================================
2025-06-16 22:41:39 | INFO     | test_scraper.py:30   | test_fed_scraper() | TESTING FEDERAL RESERVE SCRAPER
2025-06-16 22:41:39 | INFO     | test_scraper.py:31   | test_fed_scraper() | ============================================================
2025-06-16 22:41:48 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:41:48 | DEBUG    | base_scraper.py:46   | _load_last_check_time() | fed_reserve: No previous check time found
2025-06-16 22:41:48 | INFO     | test_scraper.py:42   | test_fed_scraper() | ‚úÖ Fed scraper initialized
2025-06-16 22:41:48 | INFO     | test_scraper.py:45   | test_fed_scraper() | Running Fed scraper...
2025-06-16 22:41:48 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:41:48 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: Never
2025-06-16 22:41:48 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:41:48 | INFO     | fed_scraper.py:41   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-09 22:41:48.507956
2025-06-16 22:41:48 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping press_releases
2025-06-16 22:41:49 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:41:55 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 18 items in press_releases
2025-06-16 22:41:55 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping speeches
2025-06-16 22:41:55 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping speeches: 404 Client Error: Not Found for url: https://www.federalreserve.gov/newsevents/speech.htm
2025-06-16 22:41:55 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in speeches
2025-06-16 22:41:55 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping testimony
2025-06-16 22:41:56 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:41:59 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 9 items in testimony
2025-06-16 22:41:59 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping fomc_minutes
2025-06-16 22:41:59 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping fomc_minutes: 404 Client Error: Not Found for url: https://www.federalreserve.gov/monetarypolicy/fomccalendarindex.htm
2025-06-16 22:41:59 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in fomc_minutes
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:41:59 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:41:48.507800
2025-06-16 22:41:59 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:41:59 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 27 items, saved 0 new
2025-06-16 22:41:59 | INFO     | test_scraper.py:49   | test_fed_scraper() | ‚úÖ Fed scraper SUCCESS
2025-06-16 22:41:59 | INFO     | test_scraper.py:50   | test_fed_scraper() |    New content: 0
2025-06-16 22:41:59 | INFO     | test_scraper.py:51   | test_fed_scraper() |    Total found: 27
2025-06-16 22:41:59 | INFO     | test_scraper.py:52   | test_fed_scraper() |    Execution time: 10691ms
2025-06-16 22:41:59 | INFO     | test_scraper.py:53   | test_fed_scraper() |    Message: Found 27 items, saved 0 new
2025-06-16 22:41:59 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:41:59 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Reddit WSB Scraper test...
2025-06-16 22:41:59 | INFO     | test_scraper.py:72   | test_reddit_scraper() | ============================================================
2025-06-16 22:41:59 | INFO     | test_scraper.py:73   | test_reddit_scraper() | TESTING REDDIT WSB SCRAPER
2025-06-16 22:41:59 | INFO     | test_scraper.py:74   | test_reddit_scraper() | ============================================================
2025-06-16 22:41:59 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:41:59 | DEBUG    | base_scraper.py:46   | _load_last_check_time() | reddit_wsb: No previous check time found
2025-06-16 22:41:59 | INFO     | test_scraper.py:85   | test_reddit_scraper() | ‚úÖ Reddit WSB scraper initialized
2025-06-16 22:41:59 | INFO     | test_scraper.py:88   | test_reddit_scraper() | Running Reddit WSB scraper...
2025-06-16 22:41:59 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:41:59 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: Never
2025-06-16 22:41:59 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:41:59 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-15 22:41:59.205979
2025-06-16 22:41:59 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:42:01 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:42:02 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:42:03 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:42:04 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:42:05 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:42:06 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:41:59.205864
2025-06-16 22:42:06 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:42:06 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:42:06 | INFO     | test_scraper.py:92   | test_reddit_scraper() | ‚úÖ Reddit WSB scraper SUCCESS
2025-06-16 22:42:06 | INFO     | test_scraper.py:93   | test_reddit_scraper() |    New content: 0
2025-06-16 22:42:06 | INFO     | test_scraper.py:94   | test_reddit_scraper() |    Total found: 0
2025-06-16 22:42:06 | INFO     | test_scraper.py:95   | test_reddit_scraper() |    Execution time: 7224ms
2025-06-16 22:42:06 | INFO     | test_scraper.py:96   | test_reddit_scraper() |    Message: Found 0 items, saved 0 new
2025-06-16 22:42:06 | INFO     | test_scraper.py:99   | test_reddit_scraper() | Getting trending tickers...
2025-06-16 22:42:08 | INFO     | test_scraper.py:110  | test_reddit_scraper() |    No trending tickers found
2025-06-16 22:42:08 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:42:08 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Scraper Manager test...
2025-06-16 22:42:08 | INFO     | test_scraper.py:124  | test_scraper_manager() | ============================================================
2025-06-16 22:42:08 | INFO     | test_scraper.py:125  | test_scraper_manager() | TESTING SCRAPER MANAGER
2025-06-16 22:42:08 | INFO     | test_scraper.py:126  | test_scraper_manager() | ============================================================
2025-06-16 22:42:08 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:42:08 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:41:48.507800
2025-06-16 22:42:08 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:41:59.205864
2025-06-16 22:42:08 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:42:08 | INFO     | test_scraper.py:133  | test_scraper_manager() | ‚úÖ Scraper manager initialized
2025-06-16 22:42:08 | INFO     | test_scraper.py:136  | test_scraper_manager() | Getting scraper status...
2025-06-16 22:42:08 | INFO     | test_scraper.py:138  | test_scraper_manager() |    Total scrapers: 2
2025-06-16 22:42:08 | INFO     | test_scraper.py:143  | test_scraper_manager() |    fed_reserve: Last run: 2025-06-16T22:41:48.507800, Success rate: 0.0%
2025-06-16 22:42:08 | INFO     | test_scraper.py:143  | test_scraper_manager() |    reddit_wsb: Last run: 2025-06-16T22:41:59.205864, Success rate: 0.0%
2025-06-16 22:42:08 | INFO     | test_scraper.py:146  | test_scraper_manager() | Running all scrapers in parallel...
2025-06-16 22:42:08 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:42:08 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:42:08 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:42:08 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:42:08 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:42:08 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:42:08 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:42:08 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:41:48.507800
2025-06-16 22:42:08 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:42:08 | INFO     | fed_scraper.py:41   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:41:48.507800
2025-06-16 22:42:08 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping press_releases
2025-06-16 22:42:08 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:42:08 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:41:59.205864
2025-06-16 22:42:08 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:42:08 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:11:59.205864
2025-06-16 22:42:08 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:42:08 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:42:10 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:42:11 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:42:12 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:42:13 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:42:14 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:42:15 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:42:08.595982
2025-06-16 22:42:15 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:42:15 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:42:15 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:42:15 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 18 items in press_releases
2025-06-16 22:42:15 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping speeches
2025-06-16 22:42:15 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping speeches: 404 Client Error: Not Found for url: https://www.federalreserve.gov/newsevents/speech.htm
2025-06-16 22:42:15 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in speeches
2025-06-16 22:42:15 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping testimony
2025-06-16 22:42:15 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:42:18 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 9 items in testimony
2025-06-16 22:42:18 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping fomc_minutes
2025-06-16 22:42:18 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping fomc_minutes: 404 Client Error: Not Found for url: https://www.federalreserve.gov/monetarypolicy/fomccalendarindex.htm
2025-06-16 22:42:18 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in fomc_minutes
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:18 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:42:08.594552
2025-06-16 22:42:18 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:42:18 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 27 items, saved 0 new
2025-06-16 22:42:18 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:42:18 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:42:18 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:42:18 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:42:18 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 10210ms
2025-06-16 22:42:18 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:42:18 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:42:18 | INFO     | test_scraper.py:151  | test_scraper_manager() | ‚úÖ Multi-scraper execution SUCCESS
2025-06-16 22:42:18 | INFO     | test_scraper.py:152  | test_scraper_manager() |    Successful scrapers: 2/2
2025-06-16 22:42:18 | INFO     | test_scraper.py:153  | test_scraper_manager() |    Total new content: 0
2025-06-16 22:42:18 | INFO     | test_scraper.py:154  | test_scraper_manager() |    Total execution time: 10210ms
2025-06-16 22:42:18 | INFO     | test_scraper.py:157  | test_scraper_manager() |    Per-scraper results:
2025-06-16 22:42:18 | INFO     | test_scraper.py:162  | test_scraper_manager() |      ‚úÖ reddit_wsb: 0 new items (6529ms)
2025-06-16 22:42:18 | INFO     | test_scraper.py:162  | test_scraper_manager() |      ‚úÖ fed_reserve: 0 new items (10206ms)
2025-06-16 22:42:18 | INFO     | test_scraper.py:167  | test_scraper_manager() | Getting recent content...
2025-06-16 22:42:18 | INFO     | test_scraper.py:169  | test_scraper_manager() |    Found 0 recent items
2025-06-16 22:42:18 | INFO     | test_scraper.py:177  | test_scraper_manager() | Getting trending analysis...
2025-06-16 22:42:18 | INFO     | test_scraper.py:181  | test_scraper_manager() |    Content items analyzed: 0
2025-06-16 22:42:18 | INFO     | test_scraper.py:182  | test_scraper_manager() |    Sources: []
2025-06-16 22:42:18 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:42:18 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Change Detection test...
2025-06-16 22:42:18 | INFO     | test_scraper.py:200  | test_change_detection() | ============================================================
2025-06-16 22:42:18 | INFO     | test_scraper.py:201  | test_change_detection() | TESTING CHANGE DETECTION
2025-06-16 22:42:18 | INFO     | test_scraper.py:202  | test_change_detection() | ============================================================
2025-06-16 22:42:18 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:42:18 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:42:08.594552
2025-06-16 22:42:18 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:42:08.595982
2025-06-16 22:42:18 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:42:18 | INFO     | test_scraper.py:210  | test_change_detection() | First run (should find new content)...
2025-06-16 22:42:18 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:42:18 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:42:18 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:42:18 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: False
2025-06-16 22:42:18 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:42:18 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:42:18 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:42:18 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:42:08.594552
2025-06-16 22:42:18 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: False
2025-06-16 22:42:18 | INFO     | fed_scraper.py:41   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:42:08.594552
2025-06-16 22:42:18 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping press_releases
2025-06-16 22:42:18 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:42:18 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:42:08.595982
2025-06-16 22:42:18 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: False
2025-06-16 22:42:18 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:12:08.595982
2025-06-16 22:42:18 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:42:19 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:42:20 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:42:21 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:42:23 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:42:24 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:42:24 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:42:25 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 18 items in press_releases
2025-06-16 22:42:25 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping speeches
2025-06-16 22:42:25 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping speeches: 404 Client Error: Not Found for url: https://www.federalreserve.gov/newsevents/speech.htm
2025-06-16 22:42:25 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in speeches
2025-06-16 22:42:25 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping testimony
2025-06-16 22:42:25 | INFO     | base_scraper.py:147  | run_scraping() | reddit_wsb: No new content found
2025-06-16 22:42:25 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:42:25 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:42:28 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 9 items in testimony
2025-06-16 22:42:28 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping fomc_minutes
2025-06-16 22:42:28 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping fomc_minutes: 404 Client Error: Not Found for url: https://www.federalreserve.gov/monetarypolicy/fomccalendarindex.htm
2025-06-16 22:42:28 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in fomc_minutes
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:28 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:42:18.813952
2025-06-16 22:42:28 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:42:28 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 27 items, saved 0 new
2025-06-16 22:42:28 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:42:28 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:42:28 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:42:28 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:42:28 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 9871ms
2025-06-16 22:42:28 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:42:28 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:42:28 | INFO     | test_scraper.py:214  | test_change_detection() |    First run found: 0 new items
2025-06-16 22:42:28 | INFO     | test_scraper.py:217  | test_change_detection() | Second run (should find little new content)...
2025-06-16 22:42:28 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:42:28 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:42:28 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:42:28 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: False
2025-06-16 22:42:28 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:42:28 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:42:28 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:42:28 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:42:18.813952
2025-06-16 22:42:28 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: False
2025-06-16 22:42:28 | INFO     | fed_scraper.py:41   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:42:18.813952
2025-06-16 22:42:28 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping press_releases
2025-06-16 22:42:28 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:42:28 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:42:08.595982
2025-06-16 22:42:28 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: False
2025-06-16 22:42:28 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:12:08.595982
2025-06-16 22:42:28 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:42:28 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:42:30 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:42:31 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:42:32 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:42:33 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:42:34 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:42:34 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 18 items in press_releases
2025-06-16 22:42:34 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping speeches
2025-06-16 22:42:34 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping speeches: 404 Client Error: Not Found for url: https://www.federalreserve.gov/newsevents/speech.htm
2025-06-16 22:42:34 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in speeches
2025-06-16 22:42:34 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping testimony
2025-06-16 22:42:34 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:42:35 | INFO     | base_scraper.py:147  | run_scraping() | reddit_wsb: No new content found
2025-06-16 22:42:35 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:42:37 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 9 items in testimony
2025-06-16 22:42:37 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping fomc_minutes
2025-06-16 22:42:37 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping fomc_minutes: 404 Client Error: Not Found for url: https://www.federalreserve.gov/monetarypolicy/fomccalendarindex.htm
2025-06-16 22:42:37 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in fomc_minutes
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:37 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:42:28.685191
2025-06-16 22:42:37 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:42:37 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 27 items, saved 0 new
2025-06-16 22:42:37 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:42:37 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:42:37 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:42:37 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:42:37 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 9223ms
2025-06-16 22:42:37 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:42:37 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:42:37 | INFO     | test_scraper.py:221  | test_change_detection() |    Second run found: 0 new items
2025-06-16 22:42:37 | INFO     | test_scraper.py:224  | test_change_detection() | Third run with force update (should re-scrape everything)...
2025-06-16 22:42:37 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:42:37 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:42:37 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:42:37 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:42:37 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:42:37 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:42:37 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:42:37 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:42:28.685191
2025-06-16 22:42:37 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:42:37 | INFO     | fed_scraper.py:41   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:42:28.685191
2025-06-16 22:42:37 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping press_releases
2025-06-16 22:42:37 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:42:37 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:42:08.595982
2025-06-16 22:42:37 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:42:37 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:12:08.595982
2025-06-16 22:42:37 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:42:38 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:42:39 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:42:40 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:42:41 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:42:42 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:42:43 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:42:43 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 18 items in press_releases
2025-06-16 22:42:43 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping speeches
2025-06-16 22:42:43 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping speeches: 404 Client Error: Not Found for url: https://www.federalreserve.gov/newsevents/speech.htm
2025-06-16 22:42:43 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in speeches
2025-06-16 22:42:43 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping testimony
2025-06-16 22:42:43 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:42:44 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:42:37.909818
2025-06-16 22:42:44 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:42:44 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:42:44 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:42:46 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 9 items in testimony
2025-06-16 22:42:46 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping fomc_minutes
2025-06-16 22:42:46 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping fomc_minutes: 404 Client Error: Not Found for url: https://www.federalreserve.gov/monetarypolicy/fomccalendarindex.htm
2025-06-16 22:42:46 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in fomc_minutes
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:46 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:42:37.908516
2025-06-16 22:42:46 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:42:46 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 27 items, saved 0 new
2025-06-16 22:42:46 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:42:46 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:42:46 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:42:46 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:42:46 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 8934ms
2025-06-16 22:42:46 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:42:46 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:42:46 | INFO     | test_scraper.py:228  | test_change_detection() |    Force update found: 0 new items
2025-06-16 22:42:46 | WARNING  | test_scraper.py:234  | test_change_detection() | ‚ö†Ô∏è  Change detection may not be working optimally
2025-06-16 22:42:46 | INFO     | test_scraper.py:237  | test_change_detection() | ‚úÖ Force update working: Third run re-scraped content
2025-06-16 22:42:46 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:42:46 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Database Cleanup test...
2025-06-16 22:42:46 | INFO     | test_scraper.py:250  | test_database_cleanup() | ============================================================
2025-06-16 22:42:46 | INFO     | test_scraper.py:251  | test_database_cleanup() | TESTING DATABASE CLEANUP
2025-06-16 22:42:46 | INFO     | test_scraper.py:252  | test_database_cleanup() | ============================================================
2025-06-16 22:42:46 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:42:46 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:42:37.908516
2025-06-16 22:42:46 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:42:37.909818
2025-06-16 22:42:46 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:42:46 | INFO     | test_scraper.py:261  | test_database_cleanup() | Current content items: 0
2025-06-16 22:42:46 | INFO     | test_scraper.py:264  | test_database_cleanup() | Testing cleanup with 0 days (should clean nothing)...
2025-06-16 22:42:46 | INFO     | test_scraper.py:269  | test_database_cleanup() | ‚úÖ Cleanup test successful: 0 records would be deleted
2025-06-16 22:42:46 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:42:46 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Performance Test test...
2025-06-16 22:42:46 | INFO     | test_scraper.py:282  | run_performance_test() | ============================================================
2025-06-16 22:42:46 | INFO     | test_scraper.py:283  | run_performance_test() | RUNNING PERFORMANCE TEST
2025-06-16 22:42:46 | INFO     | test_scraper.py:284  | run_performance_test() | ============================================================
2025-06-16 22:42:46 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:42:46 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:42:37.908516
2025-06-16 22:42:46 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:42:37.909818
2025-06-16 22:42:46 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:42:46 | INFO     | test_scraper.py:293  | run_performance_test() | Testing sequential execution...
2025-06-16 22:42:46 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:42:46 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:42:46 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:42:46 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:42:46 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: False
2025-06-16 22:42:46 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:42:46 | INFO     | scraper_manager.py:114  | _run_scrapers_sequential() | Running scraper: fed_reserve
2025-06-16 22:42:46 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:42:46 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:42:37.908516
2025-06-16 22:42:46 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:42:46 | INFO     | fed_scraper.py:41   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:42:37.908516
2025-06-16 22:42:46 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping press_releases
2025-06-16 22:42:47 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:42:53 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 18 items in press_releases
2025-06-16 22:42:53 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping speeches
2025-06-16 22:42:54 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping speeches: 404 Client Error: Not Found for url: https://www.federalreserve.gov/newsevents/speech.htm
2025-06-16 22:42:54 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in speeches
2025-06-16 22:42:54 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping testimony
2025-06-16 22:42:54 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:42:57 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 9 items in testimony
2025-06-16 22:42:57 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping fomc_minutes
2025-06-16 22:42:57 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping fomc_minutes: 404 Client Error: Not Found for url: https://www.federalreserve.gov/monetarypolicy/fomccalendarindex.htm
2025-06-16 22:42:57 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in fomc_minutes
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:42:57 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:42:46.860451
2025-06-16 22:42:57 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:42:57 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 27 items, saved 0 new
2025-06-16 22:42:57 | INFO     | scraper_manager.py:125  | _run_scrapers_sequential() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:42:57 | INFO     | scraper_manager.py:114  | _run_scrapers_sequential() | Running scraper: reddit_wsb
2025-06-16 22:42:57 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:42:57 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:42:37.909818
2025-06-16 22:42:57 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:42:57 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:12:37.909818
2025-06-16 22:42:57 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:42:59 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:43:00 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:43:01 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:43:02 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:43:03 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:43:04 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:42:57.476623
2025-06-16 22:43:04 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:43:04 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:43:04 | INFO     | scraper_manager.py:125  | _run_scrapers_sequential() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:43:04 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:43:04 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:43:04 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:43:04 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 17276ms
2025-06-16 22:43:04 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:43:04 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:43:04 | INFO     | test_scraper.py:298  | run_performance_test() | Testing parallel execution...
2025-06-16 22:43:04 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:43:04 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:43:04 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:43:04 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:43:04 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:43:04 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:43:04 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:43:04 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:42:46.860451
2025-06-16 22:43:04 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:43:04 | INFO     | fed_scraper.py:41   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:42:46.860451
2025-06-16 22:43:04 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping press_releases
2025-06-16 22:43:04 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:43:04 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:42:57.476623
2025-06-16 22:43:04 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:43:04 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:12:57.476623
2025-06-16 22:43:04 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:43:04 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:43:05 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:43:06 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:43:07 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:43:08 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:43:09 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:43:10 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:43:04.140504
2025-06-16 22:43:10 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:43:10 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:43:10 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:43:11 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 18 items in press_releases
2025-06-16 22:43:11 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping speeches
2025-06-16 22:43:11 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping speeches: 404 Client Error: Not Found for url: https://www.federalreserve.gov/newsevents/speech.htm
2025-06-16 22:43:11 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in speeches
2025-06-16 22:43:11 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping testimony
2025-06-16 22:43:12 | WARNING  | fed_scraper.py:352  | _get_full_content() | Fed Scraper: Could not get full content from https://www.facebook.com/federalreserve: 400 Client Error: Bad Request for url: https://www.facebook.com/federalreserve
2025-06-16 22:43:15 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 9 items in testimony
2025-06-16 22:43:15 | INFO     | fed_scraper.py:46   | scrape_new_content() | Fed Scraper: Scraping fomc_minutes
2025-06-16 22:43:15 | ERROR    | fed_scraper.py:78   | _scrape_section() | Fed Scraper: Error scraping fomc_minutes: 404 Client Error: Not Found for url: https://www.federalreserve.gov/monetarypolicy/fomccalendarindex.htm
2025-06-16 22:43:15 | INFO     | fed_scraper.py:49   | scrape_new_content() | Fed Scraper: Found 0 items in fomc_minutes
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | ERROR    | base_scraper.py:126  | _save_content() | fed_reserve: Failed to save content: DatabaseManager.save_scraped_data() got an unexpected keyword argument 'external_id'
2025-06-16 22:43:15 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:43:04.138508
2025-06-16 22:43:15 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:43:15 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 27 items, saved 0 new
2025-06-16 22:43:15 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:43:15 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:43:15 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:43:15 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:43:15 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 11387ms
2025-06-16 22:43:15 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:43:15 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:43:15 | INFO     | test_scraper.py:304  | run_performance_test() | Performance comparison:
2025-06-16 22:43:15 | INFO     | test_scraper.py:305  | run_performance_test() |    Sequential: 17.28s
2025-06-16 22:43:15 | INFO     | test_scraper.py:306  | run_performance_test() |    Parallel: 11.39s
2025-06-16 22:43:15 | INFO     | test_scraper.py:310  | run_performance_test() |    ‚úÖ Parallel execution 1.5x faster
2025-06-16 22:43:15 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:43:15 | INFO     | test_scraper.py:350  | main() | 
================================================================================
2025-06-16 22:43:15 | INFO     | test_scraper.py:351  | main() | SCRAPER SYSTEM TEST SUMMARY
2025-06-16 22:43:15 | INFO     | test_scraper.py:352  | main() | ================================================================================
2025-06-16 22:43:15 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Fed Scraper
2025-06-16 22:43:15 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Reddit WSB Scraper
2025-06-16 22:43:15 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Scraper Manager
2025-06-16 22:43:15 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Change Detection
2025-06-16 22:43:15 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Database Cleanup
2025-06-16 22:43:15 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Performance Test
2025-06-16 22:43:15 | INFO     | test_scraper.py:361  | main() | --------------------------------------------------------------------------------
2025-06-16 22:43:15 | INFO     | test_scraper.py:362  | main() | OVERALL RESULT: 6/6 tests passed
2025-06-16 22:43:15 | INFO     | test_scraper.py:365  | main() | üéâ ALL SCRAPER TESTS PASSED!
2025-06-16 22:43:15 | INFO     | test_scraper.py:366  | main() | Your multi-scraper system is working correctly.
2025-06-16 22:47:30 | INFO     | logging_config.py:93   | initialize() | ============================================================
2025-06-16 22:47:30 | INFO     | logging_config.py:94   | initialize() | UNIFIED SCREENER SYSTEM STARTING
2025-06-16 22:47:30 | INFO     | logging_config.py:95   | initialize() | Log Level: INFO
2025-06-16 22:47:30 | INFO     | logging_config.py:96   | initialize() | Log File: logs\scraper_test.log
2025-06-16 22:47:30 | INFO     | logging_config.py:97   | initialize() | Console Output: True
2025-06-16 22:47:30 | INFO     | logging_config.py:98   | initialize() | All modules will log to this unified logger
2025-06-16 22:47:30 | INFO     | logging_config.py:99   | initialize() | ============================================================
2025-06-16 22:47:30 | INFO     | test_scraper.py:324  | main() | üß™ STARTING MULTI-SCRAPER SYSTEM TESTS
2025-06-16 22:47:30 | INFO     | test_scraper.py:325  | main() | ================================================================================
2025-06-16 22:47:30 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Fed Scraper test...
2025-06-16 22:47:30 | INFO     | test_scraper.py:29   | test_fed_scraper() | ============================================================
2025-06-16 22:47:30 | INFO     | test_scraper.py:30   | test_fed_scraper() | TESTING FEDERAL RESERVE SCRAPER
2025-06-16 22:47:30 | INFO     | test_scraper.py:31   | test_fed_scraper() | ============================================================
2025-06-16 22:47:31 | ERROR    | test_scraper.py:66   | test_fed_scraper() | ‚ùå Fed scraper test crashed: No module named 'fedtools'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\tests\test_scraper.py", line 34, in test_fed_scraper
    from scrapers.fed_scraper import FedScraper
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 14, in <module>
    import fedtools.frb.releases
ModuleNotFoundError: No module named 'fedtools'
2025-06-16 22:47:31 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:47:31 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Reddit WSB Scraper test...
2025-06-16 22:47:31 | INFO     | test_scraper.py:72   | test_reddit_scraper() | ============================================================
2025-06-16 22:47:31 | INFO     | test_scraper.py:73   | test_reddit_scraper() | TESTING REDDIT WSB SCRAPER
2025-06-16 22:47:31 | INFO     | test_scraper.py:74   | test_reddit_scraper() | ============================================================
2025-06-16 22:52:28 | INFO     | logging_config.py:93   | initialize() | ============================================================
2025-06-16 22:52:28 | INFO     | logging_config.py:94   | initialize() | UNIFIED SCREENER SYSTEM STARTING
2025-06-16 22:52:28 | INFO     | logging_config.py:95   | initialize() | Log Level: INFO
2025-06-16 22:52:28 | INFO     | logging_config.py:96   | initialize() | Log File: logs\scraper_test.log
2025-06-16 22:52:28 | INFO     | logging_config.py:97   | initialize() | Console Output: True
2025-06-16 22:52:28 | INFO     | logging_config.py:98   | initialize() | All modules will log to this unified logger
2025-06-16 22:52:28 | INFO     | logging_config.py:99   | initialize() | ============================================================
2025-06-16 22:52:28 | INFO     | test_scraper.py:324  | main() | üß™ STARTING MULTI-SCRAPER SYSTEM TESTS
2025-06-16 22:52:28 | INFO     | test_scraper.py:325  | main() | ================================================================================
2025-06-16 22:52:28 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Fed Scraper test...
2025-06-16 22:52:28 | INFO     | test_scraper.py:29   | test_fed_scraper() | ============================================================
2025-06-16 22:52:28 | INFO     | test_scraper.py:30   | test_fed_scraper() | TESTING FEDERAL RESERVE SCRAPER
2025-06-16 22:52:28 | INFO     | test_scraper.py:31   | test_fed_scraper() | ============================================================
2025-06-16 22:52:34 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:52:34 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:43:04.138508
2025-06-16 22:52:34 | INFO     | fed_scraper.py:26   | __init__() | FedScraper initialized to use FedTools for scraping.
2025-06-16 22:52:34 | INFO     | fed_scraper.py:32   | __init__() | MonetaryPolicyCommittee scraper instantiated.
2025-06-16 22:52:34 | INFO     | fed_scraper.py:39   | __init__() | FederalReserveMins (FOMC) scraper instantiated.
2025-06-16 22:52:34 | INFO     | test_scraper.py:42   | test_fed_scraper() | ‚úÖ Fed scraper initialized
2025-06-16 22:52:34 | INFO     | test_scraper.py:45   | test_fed_scraper() | Running Fed scraper...
2025-06-16 22:52:34 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:52:34 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:43:04.138508
2025-06-16 22:52:34 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:52:34 | INFO     | fed_scraper.py:53   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:43:04.138508
2025-06-16 22:52:34 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:52:34 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_press_releases' for press_releases.
2025-06-16 22:52:34 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:52:34 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_speeches' for speeches.
2025-06-16 22:52:34 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:52:34 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_testimony' for testimony.
2025-06-16 22:52:34 | INFO     | fed_scraper.py:88   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:52:34 | ERROR    | fed_scraper.py:100  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_minutes'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 90, in scrape_new_content
    fomc_items = self.fomc_minutes_scraper.get_minutes()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_minutes'. Did you mean: 'find_minutes'?
2025-06-16 22:52:34 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:52:34.700760
2025-06-16 22:52:34 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:52:34 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 0 items, saved 0 new
2025-06-16 22:52:34 | INFO     | test_scraper.py:49   | test_fed_scraper() | ‚úÖ Fed scraper SUCCESS
2025-06-16 22:52:34 | INFO     | test_scraper.py:50   | test_fed_scraper() |    New content: 0
2025-06-16 22:52:34 | INFO     | test_scraper.py:51   | test_fed_scraper() |    Total found: 0
2025-06-16 22:52:34 | INFO     | test_scraper.py:52   | test_fed_scraper() |    Execution time: 6ms
2025-06-16 22:52:34 | INFO     | test_scraper.py:53   | test_fed_scraper() |    Message: Found 0 items, saved 0 new
2025-06-16 22:52:34 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:52:34 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Reddit WSB Scraper test...
2025-06-16 22:52:34 | INFO     | test_scraper.py:72   | test_reddit_scraper() | ============================================================
2025-06-16 22:52:34 | INFO     | test_scraper.py:73   | test_reddit_scraper() | TESTING REDDIT WSB SCRAPER
2025-06-16 22:52:34 | INFO     | test_scraper.py:74   | test_reddit_scraper() | ============================================================
2025-06-16 22:52:34 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:52:34 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:43:04.140504
2025-06-16 22:52:34 | INFO     | test_scraper.py:85   | test_reddit_scraper() | ‚úÖ Reddit WSB scraper initialized
2025-06-16 22:52:34 | INFO     | test_scraper.py:88   | test_reddit_scraper() | Running Reddit WSB scraper...
2025-06-16 22:52:34 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:52:34 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:43:04.140504
2025-06-16 22:52:34 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:52:34 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:13:04.140504
2025-06-16 22:52:34 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:52:36 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:52:37 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:52:39 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:52:40 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:52:41 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:52:42 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:52:34.714510
2025-06-16 22:52:42 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:52:42 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:52:42 | INFO     | test_scraper.py:92   | test_reddit_scraper() | ‚úÖ Reddit WSB scraper SUCCESS
2025-06-16 22:52:42 | INFO     | test_scraper.py:93   | test_reddit_scraper() |    New content: 0
2025-06-16 22:52:42 | INFO     | test_scraper.py:94   | test_reddit_scraper() |    Total found: 0
2025-06-16 22:52:42 | INFO     | test_scraper.py:95   | test_reddit_scraper() |    Execution time: 7480ms
2025-06-16 22:52:42 | INFO     | test_scraper.py:96   | test_reddit_scraper() |    Message: Found 0 items, saved 0 new
2025-06-16 22:52:42 | INFO     | test_scraper.py:99   | test_reddit_scraper() | Getting trending tickers...
2025-06-16 22:52:43 | INFO     | test_scraper.py:110  | test_reddit_scraper() |    No trending tickers found
2025-06-16 22:52:43 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:52:43 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Scraper Manager test...
2025-06-16 22:52:43 | INFO     | test_scraper.py:124  | test_scraper_manager() | ============================================================
2025-06-16 22:52:43 | INFO     | test_scraper.py:125  | test_scraper_manager() | TESTING SCRAPER MANAGER
2025-06-16 22:52:43 | INFO     | test_scraper.py:126  | test_scraper_manager() | ============================================================
2025-06-16 22:52:43 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:52:43 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:52:34.700760
2025-06-16 22:52:43 | INFO     | fed_scraper.py:26   | __init__() | FedScraper initialized to use FedTools for scraping.
2025-06-16 22:52:43 | INFO     | fed_scraper.py:32   | __init__() | MonetaryPolicyCommittee scraper instantiated.
2025-06-16 22:52:43 | INFO     | fed_scraper.py:39   | __init__() | FederalReserveMins (FOMC) scraper instantiated.
2025-06-16 22:52:43 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:52:34.714510
2025-06-16 22:52:43 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:52:43 | INFO     | test_scraper.py:133  | test_scraper_manager() | ‚úÖ Scraper manager initialized
2025-06-16 22:52:43 | INFO     | test_scraper.py:136  | test_scraper_manager() | Getting scraper status...
2025-06-16 22:52:43 | INFO     | test_scraper.py:138  | test_scraper_manager() |    Total scrapers: 2
2025-06-16 22:52:43 | INFO     | test_scraper.py:143  | test_scraper_manager() |    fed_reserve: Last run: 2025-06-16T22:52:34.700760, Success rate: 0.0%
2025-06-16 22:52:43 | INFO     | test_scraper.py:143  | test_scraper_manager() |    reddit_wsb: Last run: 2025-06-16T22:52:34.714510, Success rate: 0.0%
2025-06-16 22:52:43 | INFO     | test_scraper.py:146  | test_scraper_manager() | Running all scrapers in parallel...
2025-06-16 22:52:43 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:52:43 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:52:43 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:52:43 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:52:43 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:52:43 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:52:43 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:52:43 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:52:34.700760
2025-06-16 22:52:43 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:52:43 | INFO     | fed_scraper.py:53   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:52:34.700760
2025-06-16 22:52:43 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:52:43 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:52:43 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:52:34.714510
2025-06-16 22:52:43 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_press_releases' for press_releases.
2025-06-16 22:52:43 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:52:43 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:22:34.714510
2025-06-16 22:52:43 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:52:43 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:52:43 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_speeches' for speeches.
2025-06-16 22:52:43 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:52:43 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_testimony' for testimony.
2025-06-16 22:52:43 | INFO     | fed_scraper.py:88   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:52:43 | ERROR    | fed_scraper.py:100  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_minutes'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 90, in scrape_new_content
    fomc_items = self.fomc_minutes_scraper.get_minutes()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_minutes'. Did you mean: 'find_minutes'?
2025-06-16 22:52:43 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:52:43.653955
2025-06-16 22:52:43 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:52:43 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 0 items, saved 0 new
2025-06-16 22:52:43 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:52:45 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:52:46 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:52:48 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:52:49 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:52:49 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:52:50 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:52:43.654680
2025-06-16 22:52:50 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:52:50 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:52:50 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:52:50 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:52:50 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:52:50 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:52:50 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6886ms
2025-06-16 22:52:50 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:52:50 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:52:50 | INFO     | test_scraper.py:151  | test_scraper_manager() | ‚úÖ Multi-scraper execution SUCCESS
2025-06-16 22:52:50 | INFO     | test_scraper.py:152  | test_scraper_manager() |    Successful scrapers: 2/2
2025-06-16 22:52:50 | INFO     | test_scraper.py:153  | test_scraper_manager() |    Total new content: 0
2025-06-16 22:52:50 | INFO     | test_scraper.py:154  | test_scraper_manager() |    Total execution time: 6886ms
2025-06-16 22:52:50 | INFO     | test_scraper.py:157  | test_scraper_manager() |    Per-scraper results:
2025-06-16 22:52:50 | INFO     | test_scraper.py:162  | test_scraper_manager() |      ‚úÖ fed_reserve: 0 new items (11ms)
2025-06-16 22:52:50 | INFO     | test_scraper.py:162  | test_scraper_manager() |      ‚úÖ reddit_wsb: 0 new items (6883ms)
2025-06-16 22:52:50 | INFO     | test_scraper.py:167  | test_scraper_manager() | Getting recent content...
2025-06-16 22:52:50 | INFO     | test_scraper.py:169  | test_scraper_manager() |    Found 0 recent items
2025-06-16 22:52:50 | INFO     | test_scraper.py:177  | test_scraper_manager() | Getting trending analysis...
2025-06-16 22:52:50 | INFO     | test_scraper.py:181  | test_scraper_manager() |    Content items analyzed: 0
2025-06-16 22:52:50 | INFO     | test_scraper.py:182  | test_scraper_manager() |    Sources: []
2025-06-16 22:52:50 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:52:50 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Change Detection test...
2025-06-16 22:52:50 | INFO     | test_scraper.py:200  | test_change_detection() | ============================================================
2025-06-16 22:52:50 | INFO     | test_scraper.py:201  | test_change_detection() | TESTING CHANGE DETECTION
2025-06-16 22:52:50 | INFO     | test_scraper.py:202  | test_change_detection() | ============================================================
2025-06-16 22:52:50 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:52:50 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:52:43.653955
2025-06-16 22:52:50 | INFO     | fed_scraper.py:26   | __init__() | FedScraper initialized to use FedTools for scraping.
2025-06-16 22:52:50 | INFO     | fed_scraper.py:32   | __init__() | MonetaryPolicyCommittee scraper instantiated.
2025-06-16 22:52:50 | INFO     | fed_scraper.py:39   | __init__() | FederalReserveMins (FOMC) scraper instantiated.
2025-06-16 22:52:50 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:52:43.654680
2025-06-16 22:52:50 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:52:50 | INFO     | test_scraper.py:210  | test_change_detection() | First run (should find new content)...
2025-06-16 22:52:50 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:52:50 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:52:50 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:52:50 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: False
2025-06-16 22:52:50 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:52:50 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:52:50 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:52:50 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:52:43.653955
2025-06-16 22:52:50 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: False
2025-06-16 22:52:50 | INFO     | fed_scraper.py:53   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:52:43.653955
2025-06-16 22:52:50 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:52:50 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_press_releases' for press_releases.
2025-06-16 22:52:50 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:52:50 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_speeches' for speeches.
2025-06-16 22:52:50 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:52:50 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:52:50 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_testimony' for testimony.
2025-06-16 22:52:50 | INFO     | fed_scraper.py:88   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:52:50 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:52:43.654680
2025-06-16 22:52:50 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: False
2025-06-16 22:52:50 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:22:43.654680
2025-06-16 22:52:50 | ERROR    | fed_scraper.py:100  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_minutes'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 90, in scrape_new_content
    fomc_items = self.fomc_minutes_scraper.get_minutes()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_minutes'. Did you mean: 'find_minutes'?
2025-06-16 22:52:50 | INFO     | base_scraper.py:147  | run_scraping() | fed_reserve: No new content found
2025-06-16 22:52:50 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:52:50 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:52:52 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:52:53 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:52:55 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:52:56 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:52:56 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:52:57 | INFO     | base_scraper.py:147  | run_scraping() | reddit_wsb: No new content found
2025-06-16 22:52:57 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:52:57 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:52:57 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:52:57 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:52:57 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6934ms
2025-06-16 22:52:57 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:52:57 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:52:57 | INFO     | test_scraper.py:214  | test_change_detection() |    First run found: 0 new items
2025-06-16 22:52:57 | INFO     | test_scraper.py:217  | test_change_detection() | Second run (should find little new content)...
2025-06-16 22:52:57 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:52:57 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:52:57 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:52:57 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: False
2025-06-16 22:52:57 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:52:57 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:52:57 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:52:57 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:52:43.653955
2025-06-16 22:52:57 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: False
2025-06-16 22:52:57 | INFO     | fed_scraper.py:53   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:52:43.653955
2025-06-16 22:52:57 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:52:57 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_press_releases' for press_releases.
2025-06-16 22:52:57 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:52:57 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_speeches' for speeches.
2025-06-16 22:52:57 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:52:57 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:52:57 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:52:43.654680
2025-06-16 22:52:57 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_testimony' for testimony.
2025-06-16 22:52:57 | INFO     | fed_scraper.py:88   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:52:57 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: False
2025-06-16 22:52:57 | ERROR    | fed_scraper.py:100  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_minutes'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 90, in scrape_new_content
    fomc_items = self.fomc_minutes_scraper.get_minutes()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_minutes'. Did you mean: 'find_minutes'?
2025-06-16 22:52:57 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:22:43.654680
2025-06-16 22:52:57 | INFO     | base_scraper.py:147  | run_scraping() | fed_reserve: No new content found
2025-06-16 22:52:57 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:52:57 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:52:59 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:53:00 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:53:01 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:53:02 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:53:03 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:53:04 | INFO     | base_scraper.py:147  | run_scraping() | reddit_wsb: No new content found
2025-06-16 22:53:04 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:53:04 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:53:04 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:53:04 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:53:04 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6826ms
2025-06-16 22:53:04 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:53:04 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:53:04 | INFO     | test_scraper.py:221  | test_change_detection() |    Second run found: 0 new items
2025-06-16 22:53:04 | INFO     | test_scraper.py:224  | test_change_detection() | Third run with force update (should re-scrape everything)...
2025-06-16 22:53:04 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:53:04 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:53:04 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:53:04 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:53:04 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:53:04 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:53:04 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:53:04 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:52:43.653955
2025-06-16 22:53:04 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:53:04 | INFO     | fed_scraper.py:53   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:52:43.653955
2025-06-16 22:53:04 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:53:04 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:53:04 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:52:43.654680
2025-06-16 22:53:04 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_press_releases' for press_releases.
2025-06-16 22:53:04 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:53:04 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:53:04 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:22:43.654680
2025-06-16 22:53:04 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_speeches' for speeches.
2025-06-16 22:53:04 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:53:04 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:53:04 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_testimony' for testimony.
2025-06-16 22:53:04 | INFO     | fed_scraper.py:88   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:53:04 | ERROR    | fed_scraper.py:100  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_minutes'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 90, in scrape_new_content
    fomc_items = self.fomc_minutes_scraper.get_minutes()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_minutes'. Did you mean: 'find_minutes'?
2025-06-16 22:53:04 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:53:04.314332
2025-06-16 22:53:04 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:53:04 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 0 items, saved 0 new
2025-06-16 22:53:04 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:53:06 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:53:07 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:53:08 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:53:09 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:53:09 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:53:10 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:53:04.315404
2025-06-16 22:53:10 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:53:10 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:53:10 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:53:10 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:53:10 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:53:10 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:53:10 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6353ms
2025-06-16 22:53:10 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:53:10 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:53:10 | INFO     | test_scraper.py:228  | test_change_detection() |    Force update found: 0 new items
2025-06-16 22:53:10 | WARNING  | test_scraper.py:234  | test_change_detection() | ‚ö†Ô∏è  Change detection may not be working optimally
2025-06-16 22:53:10 | INFO     | test_scraper.py:237  | test_change_detection() | ‚úÖ Force update working: Third run re-scraped content
2025-06-16 22:53:10 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:53:10 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Database Cleanup test...
2025-06-16 22:53:10 | INFO     | test_scraper.py:250  | test_database_cleanup() | ============================================================
2025-06-16 22:53:10 | INFO     | test_scraper.py:251  | test_database_cleanup() | TESTING DATABASE CLEANUP
2025-06-16 22:53:10 | INFO     | test_scraper.py:252  | test_database_cleanup() | ============================================================
2025-06-16 22:53:10 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:53:10 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:53:04.314332
2025-06-16 22:53:10 | INFO     | fed_scraper.py:26   | __init__() | FedScraper initialized to use FedTools for scraping.
2025-06-16 22:53:10 | INFO     | fed_scraper.py:32   | __init__() | MonetaryPolicyCommittee scraper instantiated.
2025-06-16 22:53:10 | INFO     | fed_scraper.py:39   | __init__() | FederalReserveMins (FOMC) scraper instantiated.
2025-06-16 22:53:10 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:53:04.315404
2025-06-16 22:53:10 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:53:10 | INFO     | test_scraper.py:261  | test_database_cleanup() | Current content items: 0
2025-06-16 22:53:10 | INFO     | test_scraper.py:264  | test_database_cleanup() | Testing cleanup with 0 days (should clean nothing)...
2025-06-16 22:53:10 | INFO     | test_scraper.py:269  | test_database_cleanup() | ‚úÖ Cleanup test successful: 0 records would be deleted
2025-06-16 22:53:10 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:53:10 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Performance Test test...
2025-06-16 22:53:10 | INFO     | test_scraper.py:282  | run_performance_test() | ============================================================
2025-06-16 22:53:10 | INFO     | test_scraper.py:283  | run_performance_test() | RUNNING PERFORMANCE TEST
2025-06-16 22:53:10 | INFO     | test_scraper.py:284  | run_performance_test() | ============================================================
2025-06-16 22:53:10 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:53:10 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:53:04.314332
2025-06-16 22:53:10 | INFO     | fed_scraper.py:26   | __init__() | FedScraper initialized to use FedTools for scraping.
2025-06-16 22:53:10 | INFO     | fed_scraper.py:32   | __init__() | MonetaryPolicyCommittee scraper instantiated.
2025-06-16 22:53:10 | INFO     | fed_scraper.py:39   | __init__() | FederalReserveMins (FOMC) scraper instantiated.
2025-06-16 22:53:10 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:53:04.315404
2025-06-16 22:53:10 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:53:10 | INFO     | test_scraper.py:293  | run_performance_test() | Testing sequential execution...
2025-06-16 22:53:10 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:53:10 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:53:10 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:53:10 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:53:10 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: False
2025-06-16 22:53:10 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:53:10 | INFO     | scraper_manager.py:114  | _run_scrapers_sequential() | Running scraper: fed_reserve
2025-06-16 22:53:10 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:53:10 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:53:04.314332
2025-06-16 22:53:10 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:53:10 | INFO     | fed_scraper.py:53   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:53:04.314332
2025-06-16 22:53:10 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:53:10 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_press_releases' for press_releases.
2025-06-16 22:53:10 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:53:10 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_speeches' for speeches.
2025-06-16 22:53:10 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:53:10 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_testimony' for testimony.
2025-06-16 22:53:10 | INFO     | fed_scraper.py:88   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:53:10 | ERROR    | fed_scraper.py:100  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_minutes'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 90, in scrape_new_content
    fomc_items = self.fomc_minutes_scraper.get_minutes()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_minutes'. Did you mean: 'find_minutes'?
2025-06-16 22:53:10 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:53:10.699990
2025-06-16 22:53:10 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:53:10 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 0 items, saved 0 new
2025-06-16 22:53:10 | INFO     | scraper_manager.py:125  | _run_scrapers_sequential() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:53:10 | INFO     | scraper_manager.py:114  | _run_scrapers_sequential() | Running scraper: reddit_wsb
2025-06-16 22:53:10 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:53:10 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:53:04.315404
2025-06-16 22:53:10 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:53:10 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:23:04.315404
2025-06-16 22:53:10 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:53:12 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:53:13 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:53:15 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:53:16 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:53:16 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:53:17 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:53:10.713605
2025-06-16 22:53:17 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:53:17 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:53:17 | INFO     | scraper_manager.py:125  | _run_scrapers_sequential() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:53:17 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:53:17 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:53:17 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:53:17 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 7076ms
2025-06-16 22:53:17 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:53:17 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:53:17 | INFO     | test_scraper.py:298  | run_performance_test() | Testing parallel execution...
2025-06-16 22:53:17 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:53:17 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:53:17 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:53:17 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:53:17 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:53:17 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:53:17 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:53:17 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:53:10.699990
2025-06-16 22:53:17 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:53:17 | INFO     | fed_scraper.py:53   | scrape_new_content() | Fed Scraper: Looking for content since 2025-06-16 21:53:10.699990
2025-06-16 22:53:17 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:53:17 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:53:17 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:53:10.713605
2025-06-16 22:53:17 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_press_releases' for press_releases.
2025-06-16 22:53:17 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:53:17 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:53:17 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:23:10.713605
2025-06-16 22:53:17 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_speeches' for speeches.
2025-06-16 22:53:17 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:53:17 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:53:17 | WARNING  | fed_scraper.py:77   | scrape_new_content() | Fed Scraper: MonetaryPolicyCommittee does not have method 'get_latest_testimony' for testimony.
2025-06-16 22:53:17 | INFO     | fed_scraper.py:88   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:53:17 | ERROR    | fed_scraper.py:100  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_minutes'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 90, in scrape_new_content
    fomc_items = self.fomc_minutes_scraper.get_minutes()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_minutes'. Did you mean: 'find_minutes'?
2025-06-16 22:53:17 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:53:17.778252
2025-06-16 22:53:17 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:53:17 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 0 items, saved 0 new
2025-06-16 22:53:17 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:53:19 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:53:20 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:53:21 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:53:22 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:53:23 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:53:24 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:53:17.779120
2025-06-16 22:53:24 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:53:24 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:53:24 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:53:24 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:53:24 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:53:24 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:53:24 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6294ms
2025-06-16 22:53:24 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:53:24 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:53:24 | INFO     | test_scraper.py:304  | run_performance_test() | Performance comparison:
2025-06-16 22:53:24 | INFO     | test_scraper.py:305  | run_performance_test() |    Sequential: 7.08s
2025-06-16 22:53:24 | INFO     | test_scraper.py:306  | run_performance_test() |    Parallel: 6.30s
2025-06-16 22:53:24 | INFO     | test_scraper.py:310  | run_performance_test() |    ‚úÖ Parallel execution 1.1x faster
2025-06-16 22:53:24 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:53:24 | INFO     | test_scraper.py:350  | main() | 
================================================================================
2025-06-16 22:53:24 | INFO     | test_scraper.py:351  | main() | SCRAPER SYSTEM TEST SUMMARY
2025-06-16 22:53:24 | INFO     | test_scraper.py:352  | main() | ================================================================================
2025-06-16 22:53:24 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Fed Scraper
2025-06-16 22:53:24 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Reddit WSB Scraper
2025-06-16 22:53:24 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Scraper Manager
2025-06-16 22:53:24 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Change Detection
2025-06-16 22:53:24 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Database Cleanup
2025-06-16 22:53:24 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Performance Test
2025-06-16 22:53:24 | INFO     | test_scraper.py:361  | main() | --------------------------------------------------------------------------------
2025-06-16 22:53:24 | INFO     | test_scraper.py:362  | main() | OVERALL RESULT: 6/6 tests passed
2025-06-16 22:53:24 | INFO     | test_scraper.py:365  | main() | üéâ ALL SCRAPER TESTS PASSED!
2025-06-16 22:53:24 | INFO     | test_scraper.py:366  | main() | Your multi-scraper system is working correctly.
2025-06-16 22:54:32 | INFO     | logging_config.py:93   | initialize() | ============================================================
2025-06-16 22:54:32 | INFO     | logging_config.py:94   | initialize() | UNIFIED SCREENER SYSTEM STARTING
2025-06-16 22:54:32 | INFO     | logging_config.py:95   | initialize() | Log Level: INFO
2025-06-16 22:54:32 | INFO     | logging_config.py:96   | initialize() | Log File: logs\scraper_test.log
2025-06-16 22:54:32 | INFO     | logging_config.py:97   | initialize() | Console Output: True
2025-06-16 22:54:32 | INFO     | logging_config.py:98   | initialize() | All modules will log to this unified logger
2025-06-16 22:54:32 | INFO     | logging_config.py:99   | initialize() | ============================================================
2025-06-16 22:54:32 | INFO     | test_scraper.py:324  | main() | üß™ STARTING MULTI-SCRAPER SYSTEM TESTS
2025-06-16 22:54:32 | INFO     | test_scraper.py:325  | main() | ================================================================================
2025-06-16 22:54:32 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Fed Scraper test...
2025-06-16 22:54:32 | INFO     | test_scraper.py:29   | test_fed_scraper() | ============================================================
2025-06-16 22:54:32 | INFO     | test_scraper.py:30   | test_fed_scraper() | TESTING FEDERAL RESERVE SCRAPER
2025-06-16 22:54:32 | INFO     | test_scraper.py:31   | test_fed_scraper() | ============================================================
2025-06-16 22:54:38 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:54:38 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:53:17.778252
2025-06-16 22:54:38 | INFO     | fed_scraper.py:23   | __init__() | FedScraper initialized to use FedTools for scraping.
2025-06-16 22:54:38 | INFO     | fed_scraper.py:29   | __init__() | MonetaryPolicyCommittee scraper instantiated.
2025-06-16 22:54:38 | INFO     | fed_scraper.py:36   | __init__() | FederalReserveMins (FOMC) scraper instantiated.
2025-06-16 22:54:38 | INFO     | test_scraper.py:42   | test_fed_scraper() | ‚úÖ Fed scraper initialized
2025-06-16 22:54:38 | INFO     | test_scraper.py:45   | test_fed_scraper() | Running Fed scraper...
2025-06-16 22:54:38 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:54:38 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:53:17.778252
2025-06-16 22:54:38 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:54:38 | INFO     | fed_scraper.py:54   | scrape_new_content() | Fed Scraper: Looking for content from 2025-06-16 to 2025-06-16
2025-06-16 22:54:38 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:54:38 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape press_releases with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:54:38 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:54:38 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape speeches with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:54:38 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:54:38 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape testimony with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:54:38 | INFO     | fed_scraper.py:92   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:54:38 | ERROR    | fed_scraper.py:110  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 95, in scrape_new_content
    dataset = self.fomc_minutes_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_contents'
2025-06-16 22:54:38 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:54:38.822465
2025-06-16 22:54:38 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:54:38 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 0 items, saved 0 new
2025-06-16 22:54:38 | INFO     | test_scraper.py:49   | test_fed_scraper() | ‚úÖ Fed scraper SUCCESS
2025-06-16 22:54:38 | INFO     | test_scraper.py:50   | test_fed_scraper() |    New content: 0
2025-06-16 22:54:38 | INFO     | test_scraper.py:51   | test_fed_scraper() |    Total found: 0
2025-06-16 22:54:38 | INFO     | test_scraper.py:52   | test_fed_scraper() |    Execution time: 8ms
2025-06-16 22:54:38 | INFO     | test_scraper.py:53   | test_fed_scraper() |    Message: Found 0 items, saved 0 new
2025-06-16 22:54:38 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:54:38 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Reddit WSB Scraper test...
2025-06-16 22:54:38 | INFO     | test_scraper.py:72   | test_reddit_scraper() | ============================================================
2025-06-16 22:54:38 | INFO     | test_scraper.py:73   | test_reddit_scraper() | TESTING REDDIT WSB SCRAPER
2025-06-16 22:54:38 | INFO     | test_scraper.py:74   | test_reddit_scraper() | ============================================================
2025-06-16 22:54:38 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:54:38 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:53:17.779120
2025-06-16 22:54:38 | INFO     | test_scraper.py:85   | test_reddit_scraper() | ‚úÖ Reddit WSB scraper initialized
2025-06-16 22:54:38 | INFO     | test_scraper.py:88   | test_reddit_scraper() | Running Reddit WSB scraper...
2025-06-16 22:54:38 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:54:38 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:53:17.779120
2025-06-16 22:54:38 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:54:38 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:23:17.779120
2025-06-16 22:54:38 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:54:40 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:54:41 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:54:42 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:54:43 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:54:44 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:54:45 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:54:38.837033
2025-06-16 22:54:45 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:54:45 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:54:45 | INFO     | test_scraper.py:92   | test_reddit_scraper() | ‚úÖ Reddit WSB scraper SUCCESS
2025-06-16 22:54:45 | INFO     | test_scraper.py:93   | test_reddit_scraper() |    New content: 0
2025-06-16 22:54:45 | INFO     | test_scraper.py:94   | test_reddit_scraper() |    Total found: 0
2025-06-16 22:54:45 | INFO     | test_scraper.py:95   | test_reddit_scraper() |    Execution time: 6520ms
2025-06-16 22:54:45 | INFO     | test_scraper.py:96   | test_reddit_scraper() |    Message: Found 0 items, saved 0 new
2025-06-16 22:54:45 | INFO     | test_scraper.py:99   | test_reddit_scraper() | Getting trending tickers...
2025-06-16 22:54:47 | INFO     | test_scraper.py:110  | test_reddit_scraper() |    No trending tickers found
2025-06-16 22:54:47 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:54:47 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Scraper Manager test...
2025-06-16 22:54:47 | INFO     | test_scraper.py:124  | test_scraper_manager() | ============================================================
2025-06-16 22:54:47 | INFO     | test_scraper.py:125  | test_scraper_manager() | TESTING SCRAPER MANAGER
2025-06-16 22:54:47 | INFO     | test_scraper.py:126  | test_scraper_manager() | ============================================================
2025-06-16 22:54:47 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:54:47 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:54:38.822465
2025-06-16 22:54:47 | INFO     | fed_scraper.py:23   | __init__() | FedScraper initialized to use FedTools for scraping.
2025-06-16 22:54:47 | INFO     | fed_scraper.py:29   | __init__() | MonetaryPolicyCommittee scraper instantiated.
2025-06-16 22:54:47 | INFO     | fed_scraper.py:36   | __init__() | FederalReserveMins (FOMC) scraper instantiated.
2025-06-16 22:54:47 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:54:38.837033
2025-06-16 22:54:47 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:54:47 | INFO     | test_scraper.py:133  | test_scraper_manager() | ‚úÖ Scraper manager initialized
2025-06-16 22:54:47 | INFO     | test_scraper.py:136  | test_scraper_manager() | Getting scraper status...
2025-06-16 22:54:47 | INFO     | test_scraper.py:138  | test_scraper_manager() |    Total scrapers: 2
2025-06-16 22:54:47 | INFO     | test_scraper.py:143  | test_scraper_manager() |    fed_reserve: Last run: 2025-06-16T22:54:38.822465, Success rate: 0.0%
2025-06-16 22:54:47 | INFO     | test_scraper.py:143  | test_scraper_manager() |    reddit_wsb: Last run: 2025-06-16T22:54:38.837033, Success rate: 0.0%
2025-06-16 22:54:47 | INFO     | test_scraper.py:146  | test_scraper_manager() | Running all scrapers in parallel...
2025-06-16 22:54:47 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:54:47 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:54:47 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:54:47 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:54:47 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:54:47 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:54:47 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:54:47 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:54:38.822465
2025-06-16 22:54:47 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:54:47 | INFO     | fed_scraper.py:54   | scrape_new_content() | Fed Scraper: Looking for content from 2025-06-16 to 2025-06-16
2025-06-16 22:54:47 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:54:47 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape press_releases with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:54:47 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:54:47 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:54:47 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape speeches with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:54:47 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:54:47 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape testimony with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:54:47 | INFO     | fed_scraper.py:92   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:54:47 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:54:38.837033
2025-06-16 22:54:47 | ERROR    | fed_scraper.py:110  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 95, in scrape_new_content
    dataset = self.fomc_minutes_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_contents'
2025-06-16 22:54:47 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:54:47 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:24:38.837033
2025-06-16 22:54:47 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:54:47 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:54:47.482289
2025-06-16 22:54:47 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:54:47 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 0 items, saved 0 new
2025-06-16 22:54:47 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:54:49 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:54:50 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:54:51 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:54:52 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:54:52 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:54:53 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:54:47.483345
2025-06-16 22:54:53 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:54:53 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:54:53 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:54:53 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:54:53 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:54:53 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:54:53 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6121ms
2025-06-16 22:54:53 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:54:53 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:54:53 | INFO     | test_scraper.py:151  | test_scraper_manager() | ‚úÖ Multi-scraper execution SUCCESS
2025-06-16 22:54:53 | INFO     | test_scraper.py:152  | test_scraper_manager() |    Successful scrapers: 2/2
2025-06-16 22:54:53 | INFO     | test_scraper.py:153  | test_scraper_manager() |    Total new content: 0
2025-06-16 22:54:53 | INFO     | test_scraper.py:154  | test_scraper_manager() |    Total execution time: 6121ms
2025-06-16 22:54:53 | INFO     | test_scraper.py:157  | test_scraper_manager() |    Per-scraper results:
2025-06-16 22:54:53 | INFO     | test_scraper.py:162  | test_scraper_manager() |      ‚úÖ fed_reserve: 0 new items (12ms)
2025-06-16 22:54:53 | INFO     | test_scraper.py:162  | test_scraper_manager() |      ‚úÖ reddit_wsb: 0 new items (6118ms)
2025-06-16 22:54:53 | INFO     | test_scraper.py:167  | test_scraper_manager() | Getting recent content...
2025-06-16 22:54:53 | INFO     | test_scraper.py:169  | test_scraper_manager() |    Found 0 recent items
2025-06-16 22:54:53 | INFO     | test_scraper.py:177  | test_scraper_manager() | Getting trending analysis...
2025-06-16 22:54:53 | INFO     | test_scraper.py:181  | test_scraper_manager() |    Content items analyzed: 0
2025-06-16 22:54:53 | INFO     | test_scraper.py:182  | test_scraper_manager() |    Sources: []
2025-06-16 22:54:53 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:54:53 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Change Detection test...
2025-06-16 22:54:53 | INFO     | test_scraper.py:200  | test_change_detection() | ============================================================
2025-06-16 22:54:53 | INFO     | test_scraper.py:201  | test_change_detection() | TESTING CHANGE DETECTION
2025-06-16 22:54:53 | INFO     | test_scraper.py:202  | test_change_detection() | ============================================================
2025-06-16 22:54:53 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:54:53 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:54:47.482289
2025-06-16 22:54:53 | INFO     | fed_scraper.py:23   | __init__() | FedScraper initialized to use FedTools for scraping.
2025-06-16 22:54:53 | INFO     | fed_scraper.py:29   | __init__() | MonetaryPolicyCommittee scraper instantiated.
2025-06-16 22:54:53 | INFO     | fed_scraper.py:36   | __init__() | FederalReserveMins (FOMC) scraper instantiated.
2025-06-16 22:54:53 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:54:47.483345
2025-06-16 22:54:53 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:54:53 | INFO     | test_scraper.py:210  | test_change_detection() | First run (should find new content)...
2025-06-16 22:54:53 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:54:53 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:54:53 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:54:53 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: False
2025-06-16 22:54:53 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:54:53 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:54:53 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:54:53 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:54:47.482289
2025-06-16 22:54:53 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: False
2025-06-16 22:54:53 | INFO     | fed_scraper.py:54   | scrape_new_content() | Fed Scraper: Looking for content from 2025-06-16 to 2025-06-16
2025-06-16 22:54:53 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:54:53 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape press_releases with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:54:53 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:54:53 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:54:53 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape speeches with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:54:53 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:54:53 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:54:47.483345
2025-06-16 22:54:53 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: False
2025-06-16 22:54:53 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape testimony with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:54:53 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:24:47.483345
2025-06-16 22:54:53 | INFO     | fed_scraper.py:92   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:54:53 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:54:53 | ERROR    | fed_scraper.py:110  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 95, in scrape_new_content
    dataset = self.fomc_minutes_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_contents'
2025-06-16 22:54:53 | INFO     | base_scraper.py:147  | run_scraping() | fed_reserve: No new content found
2025-06-16 22:54:53 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:54:55 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:54:56 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:54:57 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:54:58 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:54:59 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:55:00 | INFO     | base_scraper.py:147  | run_scraping() | reddit_wsb: No new content found
2025-06-16 22:55:00 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:55:00 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:55:00 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:55:00 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:55:00 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6401ms
2025-06-16 22:55:00 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:55:00 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:55:00 | INFO     | test_scraper.py:214  | test_change_detection() |    First run found: 0 new items
2025-06-16 22:55:00 | INFO     | test_scraper.py:217  | test_change_detection() | Second run (should find little new content)...
2025-06-16 22:55:00 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:55:00 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:55:00 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:55:00 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: False
2025-06-16 22:55:00 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:55:00 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:55:00 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:55:00 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:54:47.482289
2025-06-16 22:55:00 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: False
2025-06-16 22:55:00 | INFO     | fed_scraper.py:54   | scrape_new_content() | Fed Scraper: Looking for content from 2025-06-16 to 2025-06-16
2025-06-16 22:55:00 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:55:00 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape press_releases with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:55:00 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:55:00 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:55:00 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:54:47.483345
2025-06-16 22:55:00 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape speeches with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:55:00 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: False
2025-06-16 22:55:00 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:55:00 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:24:47.483345
2025-06-16 22:55:00 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:55:00 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape testimony with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:55:00 | INFO     | fed_scraper.py:92   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:55:00 | ERROR    | fed_scraper.py:110  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 95, in scrape_new_content
    dataset = self.fomc_minutes_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_contents'
2025-06-16 22:55:00 | INFO     | base_scraper.py:147  | run_scraping() | fed_reserve: No new content found
2025-06-16 22:55:00 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:55:01 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:55:02 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:55:03 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:55:04 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:55:05 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:55:06 | INFO     | base_scraper.py:147  | run_scraping() | reddit_wsb: No new content found
2025-06-16 22:55:06 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:55:06 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:55:06 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:55:06 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:55:06 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6244ms
2025-06-16 22:55:06 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:55:06 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:55:06 | INFO     | test_scraper.py:221  | test_change_detection() |    Second run found: 0 new items
2025-06-16 22:55:06 | INFO     | test_scraper.py:224  | test_change_detection() | Third run with force update (should re-scrape everything)...
2025-06-16 22:55:06 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:55:06 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:55:06 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:55:06 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:55:06 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:55:06 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:55:06 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:55:06 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:54:47.482289
2025-06-16 22:55:06 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:55:06 | INFO     | fed_scraper.py:54   | scrape_new_content() | Fed Scraper: Looking for content from 2025-06-16 to 2025-06-16
2025-06-16 22:55:06 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:55:06 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape press_releases with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:55:06 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:55:06 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:55:06 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:54:47.483345
2025-06-16 22:55:06 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape speeches with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:55:06 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:55:06 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:55:06 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:24:47.483345
2025-06-16 22:55:06 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:55:06 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape testimony with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:55:06 | INFO     | fed_scraper.py:92   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:55:06 | ERROR    | fed_scraper.py:110  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 95, in scrape_new_content
    dataset = self.fomc_minutes_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_contents'
2025-06-16 22:55:06 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:55:06.259426
2025-06-16 22:55:06 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:55:06 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 0 items, saved 0 new
2025-06-16 22:55:06 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:55:07 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:55:08 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:55:10 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:55:11 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:55:11 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:55:12 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:55:06.261382
2025-06-16 22:55:12 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:55:12 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:55:12 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:55:12 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:55:12 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:55:12 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:55:12 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6537ms
2025-06-16 22:55:12 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:55:12 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:55:12 | INFO     | test_scraper.py:228  | test_change_detection() |    Force update found: 0 new items
2025-06-16 22:55:12 | WARNING  | test_scraper.py:234  | test_change_detection() | ‚ö†Ô∏è  Change detection may not be working optimally
2025-06-16 22:55:12 | INFO     | test_scraper.py:237  | test_change_detection() | ‚úÖ Force update working: Third run re-scraped content
2025-06-16 22:55:12 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:55:12 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Database Cleanup test...
2025-06-16 22:55:12 | INFO     | test_scraper.py:250  | test_database_cleanup() | ============================================================
2025-06-16 22:55:12 | INFO     | test_scraper.py:251  | test_database_cleanup() | TESTING DATABASE CLEANUP
2025-06-16 22:55:12 | INFO     | test_scraper.py:252  | test_database_cleanup() | ============================================================
2025-06-16 22:55:12 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:55:12 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:06.259426
2025-06-16 22:55:12 | INFO     | fed_scraper.py:23   | __init__() | FedScraper initialized to use FedTools for scraping.
2025-06-16 22:55:12 | INFO     | fed_scraper.py:29   | __init__() | MonetaryPolicyCommittee scraper instantiated.
2025-06-16 22:55:12 | INFO     | fed_scraper.py:36   | __init__() | FederalReserveMins (FOMC) scraper instantiated.
2025-06-16 22:55:12 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:55:06.261382
2025-06-16 22:55:12 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:55:12 | INFO     | test_scraper.py:261  | test_database_cleanup() | Current content items: 0
2025-06-16 22:55:12 | INFO     | test_scraper.py:264  | test_database_cleanup() | Testing cleanup with 0 days (should clean nothing)...
2025-06-16 22:55:12 | INFO     | test_scraper.py:269  | test_database_cleanup() | ‚úÖ Cleanup test successful: 0 records would be deleted
2025-06-16 22:55:12 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:55:12 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Performance Test test...
2025-06-16 22:55:12 | INFO     | test_scraper.py:282  | run_performance_test() | ============================================================
2025-06-16 22:55:12 | INFO     | test_scraper.py:283  | run_performance_test() | RUNNING PERFORMANCE TEST
2025-06-16 22:55:12 | INFO     | test_scraper.py:284  | run_performance_test() | ============================================================
2025-06-16 22:55:12 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:55:12 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:06.259426
2025-06-16 22:55:12 | INFO     | fed_scraper.py:23   | __init__() | FedScraper initialized to use FedTools for scraping.
2025-06-16 22:55:12 | INFO     | fed_scraper.py:29   | __init__() | MonetaryPolicyCommittee scraper instantiated.
2025-06-16 22:55:12 | INFO     | fed_scraper.py:36   | __init__() | FederalReserveMins (FOMC) scraper instantiated.
2025-06-16 22:55:12 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:55:06.261382
2025-06-16 22:55:12 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:55:12 | INFO     | test_scraper.py:293  | run_performance_test() | Testing sequential execution...
2025-06-16 22:55:12 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:55:12 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:55:12 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:55:12 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:55:12 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: False
2025-06-16 22:55:12 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:55:12 | INFO     | scraper_manager.py:114  | _run_scrapers_sequential() | Running scraper: fed_reserve
2025-06-16 22:55:12 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:55:12 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:06.259426
2025-06-16 22:55:12 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:55:12 | INFO     | fed_scraper.py:54   | scrape_new_content() | Fed Scraper: Looking for content from 2025-06-16 to 2025-06-16
2025-06-16 22:55:12 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:55:12 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape press_releases with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:55:12 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:55:12 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape speeches with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:55:12 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:55:12 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape testimony with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:55:12 | INFO     | fed_scraper.py:92   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:55:12 | ERROR    | fed_scraper.py:110  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 95, in scrape_new_content
    dataset = self.fomc_minutes_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_contents'
2025-06-16 22:55:12 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:55:12.822955
2025-06-16 22:55:12 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:55:12 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 0 items, saved 0 new
2025-06-16 22:55:12 | INFO     | scraper_manager.py:125  | _run_scrapers_sequential() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:55:12 | INFO     | scraper_manager.py:114  | _run_scrapers_sequential() | Running scraper: reddit_wsb
2025-06-16 22:55:12 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:55:12 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:55:06.261382
2025-06-16 22:55:12 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:55:12 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:25:06.261382
2025-06-16 22:55:12 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:55:14 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:55:15 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:55:16 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:55:17 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:55:18 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:55:19 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:55:12.835593
2025-06-16 22:55:19 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:55:19 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:55:19 | INFO     | scraper_manager.py:125  | _run_scrapers_sequential() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:55:19 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:55:19 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:55:19 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:55:19 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6458ms
2025-06-16 22:55:19 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:55:19 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:55:19 | INFO     | test_scraper.py:298  | run_performance_test() | Testing parallel execution...
2025-06-16 22:55:19 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:55:19 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:55:19 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:55:19 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:55:19 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:55:19 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:55:19 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:55:19 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:12.822955
2025-06-16 22:55:19 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:55:19 | INFO     | fed_scraper.py:54   | scrape_new_content() | Fed Scraper: Looking for content from 2025-06-16 to 2025-06-16
2025-06-16 22:55:19 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping press releases with FedTools...
2025-06-16 22:55:19 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:55:19 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape press_releases with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:55:19 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:55:12.835593
2025-06-16 22:55:19 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping speeches with FedTools...
2025-06-16 22:55:19 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:55:19 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape speeches with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:55:19 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:25:12.835593
2025-06-16 22:55:19 | INFO     | fed_scraper.py:67   | scrape_new_content() | Fed Scraper: Scraping testimony with FedTools...
2025-06-16 22:55:19 | ERROR    | fed_scraper.py:84   | scrape_new_content() | Fed Scraper: Failed to scrape testimony with FedTools: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 70, in scrape_new_content
    dataset = self.mpc_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MonetaryPolicyCommittee' object has no attribute 'get_contents'
2025-06-16 22:55:19 | INFO     | fed_scraper.py:92   | scrape_new_content() | Fed Scraper: Scraping FOMC minutes with FedTools...
2025-06-16 22:55:19 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:55:19 | ERROR    | fed_scraper.py:110  | scrape_new_content() | Fed Scraper: Failed to scrape FOMC minutes with FedTools: 'FederalReserveMins' object has no attribute 'get_contents'
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 95, in scrape_new_content
    dataset = self.fomc_minutes_scraper.get_contents(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'FederalReserveMins' object has no attribute 'get_contents'
2025-06-16 22:55:19 | DEBUG    | base_scraper.py:75   | _save_check_time() | fed_reserve: Check time saved: 2025-06-16 22:55:19.283048
2025-06-16 22:55:19 | INFO     | base_scraper.py:186  | run_scraping() | fed_reserve: Scraping completed successfully
2025-06-16 22:55:19 | INFO     | base_scraper.py:187  | run_scraping() | fed_reserve: Found 0 items, saved 0 new
2025-06-16 22:55:19 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:55:21 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:55:22 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:55:23 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:55:24 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:55:24 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:55:25 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:55:19.284343
2025-06-16 22:55:25 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:55:25 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:55:25 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:55:25 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:55:25 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:55:25 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:55:25 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6467ms
2025-06-16 22:55:25 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 2/2
2025-06-16 22:55:25 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:55:25 | INFO     | test_scraper.py:304  | run_performance_test() | Performance comparison:
2025-06-16 22:55:25 | INFO     | test_scraper.py:305  | run_performance_test() |    Sequential: 6.46s
2025-06-16 22:55:25 | INFO     | test_scraper.py:306  | run_performance_test() |    Parallel: 6.47s
2025-06-16 22:55:25 | INFO     | test_scraper.py:312  | run_performance_test() |    ‚ö†Ô∏è  No significant performance improvement from parallel execution
2025-06-16 22:55:25 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:55:25 | INFO     | test_scraper.py:350  | main() | 
================================================================================
2025-06-16 22:55:25 | INFO     | test_scraper.py:351  | main() | SCRAPER SYSTEM TEST SUMMARY
2025-06-16 22:55:25 | INFO     | test_scraper.py:352  | main() | ================================================================================
2025-06-16 22:55:25 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Fed Scraper
2025-06-16 22:55:25 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Reddit WSB Scraper
2025-06-16 22:55:25 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Scraper Manager
2025-06-16 22:55:25 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Change Detection
2025-06-16 22:55:25 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Database Cleanup
2025-06-16 22:55:25 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Performance Test
2025-06-16 22:55:25 | INFO     | test_scraper.py:361  | main() | --------------------------------------------------------------------------------
2025-06-16 22:55:25 | INFO     | test_scraper.py:362  | main() | OVERALL RESULT: 6/6 tests passed
2025-06-16 22:55:25 | INFO     | test_scraper.py:365  | main() | üéâ ALL SCRAPER TESTS PASSED!
2025-06-16 22:55:25 | INFO     | test_scraper.py:366  | main() | Your multi-scraper system is working correctly.
2025-06-16 22:56:41 | INFO     | logging_config.py:93   | initialize() | ============================================================
2025-06-16 22:56:41 | INFO     | logging_config.py:94   | initialize() | UNIFIED SCREENER SYSTEM STARTING
2025-06-16 22:56:41 | INFO     | logging_config.py:95   | initialize() | Log Level: INFO
2025-06-16 22:56:41 | INFO     | logging_config.py:96   | initialize() | Log File: logs\scraper_test.log
2025-06-16 22:56:41 | INFO     | logging_config.py:97   | initialize() | Console Output: True
2025-06-16 22:56:41 | INFO     | logging_config.py:98   | initialize() | All modules will log to this unified logger
2025-06-16 22:56:41 | INFO     | logging_config.py:99   | initialize() | ============================================================
2025-06-16 22:56:41 | INFO     | test_scraper.py:324  | main() | üß™ STARTING MULTI-SCRAPER SYSTEM TESTS
2025-06-16 22:56:41 | INFO     | test_scraper.py:325  | main() | ================================================================================
2025-06-16 22:56:41 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Fed Scraper test...
2025-06-16 22:56:41 | INFO     | test_scraper.py:29   | test_fed_scraper() | ============================================================
2025-06-16 22:56:41 | INFO     | test_scraper.py:30   | test_fed_scraper() | TESTING FEDERAL RESERVE SCRAPER
2025-06-16 22:56:41 | INFO     | test_scraper.py:31   | test_fed_scraper() | ============================================================
2025-06-16 22:56:47 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:56:47 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 22:56:47 | INFO     | test_scraper.py:42   | test_fed_scraper() | ‚úÖ Fed scraper initialized
2025-06-16 22:56:47 | INFO     | test_scraper.py:45   | test_fed_scraper() | Running Fed scraper...
2025-06-16 22:56:47 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:56:47 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:56:47 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:57:49 | INFO     | logging_config.py:93   | initialize() | ============================================================
2025-06-16 22:57:49 | INFO     | logging_config.py:94   | initialize() | UNIFIED SCREENER SYSTEM STARTING
2025-06-16 22:57:49 | INFO     | logging_config.py:95   | initialize() | Log Level: INFO
2025-06-16 22:57:49 | INFO     | logging_config.py:96   | initialize() | Log File: logs\scraper_test.log
2025-06-16 22:57:49 | INFO     | logging_config.py:97   | initialize() | Console Output: True
2025-06-16 22:57:49 | INFO     | logging_config.py:98   | initialize() | All modules will log to this unified logger
2025-06-16 22:57:49 | INFO     | logging_config.py:99   | initialize() | ============================================================
2025-06-16 22:57:49 | INFO     | test_scraper.py:324  | main() | üß™ STARTING MULTI-SCRAPER SYSTEM TESTS
2025-06-16 22:57:49 | INFO     | test_scraper.py:325  | main() | ================================================================================
2025-06-16 22:57:49 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Fed Scraper test...
2025-06-16 22:57:49 | INFO     | test_scraper.py:29   | test_fed_scraper() | ============================================================
2025-06-16 22:57:49 | INFO     | test_scraper.py:30   | test_fed_scraper() | TESTING FEDERAL RESERVE SCRAPER
2025-06-16 22:57:49 | INFO     | test_scraper.py:31   | test_fed_scraper() | ============================================================
2025-06-16 22:57:56 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:57:56 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 22:57:56 | INFO     | test_scraper.py:42   | test_fed_scraper() | ‚úÖ Fed scraper initialized
2025-06-16 22:57:56 | INFO     | test_scraper.py:45   | test_fed_scraper() | Running Fed scraper...
2025-06-16 22:57:56 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:57:56 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:57:56 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:57:56 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The 'start_year' argument and 'historical_split' argument must be in the past.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2025, verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 48, in __init__
    raise ValueError("The 'start_year' argument and 'historical_split' argument must be in the past.")
ValueError: The 'start_year' argument and 'historical_split' argument must be in the past.
2025-06-16 22:57:56 | ERROR    | test_scraper.py:61   | test_fed_scraper() | ‚ùå Fed scraper FAILED: The 'start_year' argument and 'historical_split' argument must be in the past.
2025-06-16 22:57:56 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:57:56 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Reddit WSB Scraper test...
2025-06-16 22:57:56 | INFO     | test_scraper.py:72   | test_reddit_scraper() | ============================================================
2025-06-16 22:57:56 | INFO     | test_scraper.py:73   | test_reddit_scraper() | TESTING REDDIT WSB SCRAPER
2025-06-16 22:57:56 | INFO     | test_scraper.py:74   | test_reddit_scraper() | ============================================================
2025-06-16 22:57:56 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:57:56 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:55:19.284343
2025-06-16 22:57:56 | INFO     | test_scraper.py:85   | test_reddit_scraper() | ‚úÖ Reddit WSB scraper initialized
2025-06-16 22:57:56 | INFO     | test_scraper.py:88   | test_reddit_scraper() | Running Reddit WSB scraper...
2025-06-16 22:57:56 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:57:56 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:55:19.284343
2025-06-16 22:57:56 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:57:56 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:25:19.284343
2025-06-16 22:57:56 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:57:59 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:58:00 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:58:02 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:58:03 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:58:03 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:58:04 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:57:56.834937
2025-06-16 22:58:04 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:58:04 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:58:04 | INFO     | test_scraper.py:92   | test_reddit_scraper() | ‚úÖ Reddit WSB scraper SUCCESS
2025-06-16 22:58:04 | INFO     | test_scraper.py:93   | test_reddit_scraper() |    New content: 0
2025-06-16 22:58:04 | INFO     | test_scraper.py:94   | test_reddit_scraper() |    Total found: 0
2025-06-16 22:58:04 | INFO     | test_scraper.py:95   | test_reddit_scraper() |    Execution time: 7682ms
2025-06-16 22:58:04 | INFO     | test_scraper.py:96   | test_reddit_scraper() |    Message: Found 0 items, saved 0 new
2025-06-16 22:58:04 | INFO     | test_scraper.py:99   | test_reddit_scraper() | Getting trending tickers...
2025-06-16 22:58:06 | INFO     | test_scraper.py:110  | test_reddit_scraper() |    No trending tickers found
2025-06-16 22:58:06 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:58:06 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Scraper Manager test...
2025-06-16 22:58:06 | INFO     | test_scraper.py:124  | test_scraper_manager() | ============================================================
2025-06-16 22:58:06 | INFO     | test_scraper.py:125  | test_scraper_manager() | TESTING SCRAPER MANAGER
2025-06-16 22:58:06 | INFO     | test_scraper.py:126  | test_scraper_manager() | ============================================================
2025-06-16 22:58:06 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:58:06 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 22:58:06 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:57:56.834937
2025-06-16 22:58:06 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:58:06 | INFO     | test_scraper.py:133  | test_scraper_manager() | ‚úÖ Scraper manager initialized
2025-06-16 22:58:06 | INFO     | test_scraper.py:136  | test_scraper_manager() | Getting scraper status...
2025-06-16 22:58:06 | INFO     | test_scraper.py:138  | test_scraper_manager() |    Total scrapers: 2
2025-06-16 22:58:06 | INFO     | test_scraper.py:143  | test_scraper_manager() |    fed_reserve: Last run: 2025-06-16T22:55:19.283048, Success rate: 0.0%
2025-06-16 22:58:06 | INFO     | test_scraper.py:143  | test_scraper_manager() |    reddit_wsb: Last run: 2025-06-16T22:57:56.834937, Success rate: 0.0%
2025-06-16 22:58:06 | INFO     | test_scraper.py:146  | test_scraper_manager() | Running all scrapers in parallel...
2025-06-16 22:58:06 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:58:06 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:58:06 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:58:06 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:58:06 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:58:06 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:58:06 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:58:06 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:58:06 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:58:06 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The 'start_year' argument and 'historical_split' argument must be in the past.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2025, verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 48, in __init__
    raise ValueError("The 'start_year' argument and 'historical_split' argument must be in the past.")
ValueError: The 'start_year' argument and 'historical_split' argument must be in the past.
2025-06-16 22:58:06 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:58:06 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:57:56.834937
2025-06-16 22:58:06 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 22:58:06 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:58:06 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:27:56.834937
2025-06-16 22:58:06 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:58:08 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:58:09 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:58:10 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:58:11 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:58:12 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:58:13 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:58:06.427526
2025-06-16 22:58:13 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:58:13 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:58:13 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:58:13 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:58:13 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:58:13 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:58:13 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6714ms
2025-06-16 22:58:13 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 22:58:13 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:58:13 | INFO     | test_scraper.py:151  | test_scraper_manager() | ‚úÖ Multi-scraper execution SUCCESS
2025-06-16 22:58:13 | INFO     | test_scraper.py:152  | test_scraper_manager() |    Successful scrapers: 1/2
2025-06-16 22:58:13 | INFO     | test_scraper.py:153  | test_scraper_manager() |    Total new content: 0
2025-06-16 22:58:13 | INFO     | test_scraper.py:154  | test_scraper_manager() |    Total execution time: 6714ms
2025-06-16 22:58:13 | INFO     | test_scraper.py:157  | test_scraper_manager() |    Per-scraper results:
2025-06-16 22:58:13 | INFO     | test_scraper.py:162  | test_scraper_manager() |      ‚ùå fed_reserve: 0 new items (2ms)
2025-06-16 22:58:13 | INFO     | test_scraper.py:162  | test_scraper_manager() |      ‚úÖ reddit_wsb: 0 new items (6710ms)
2025-06-16 22:58:13 | INFO     | test_scraper.py:167  | test_scraper_manager() | Getting recent content...
2025-06-16 22:58:13 | INFO     | test_scraper.py:169  | test_scraper_manager() |    Found 0 recent items
2025-06-16 22:58:13 | INFO     | test_scraper.py:177  | test_scraper_manager() | Getting trending analysis...
2025-06-16 22:58:13 | INFO     | test_scraper.py:181  | test_scraper_manager() |    Content items analyzed: 0
2025-06-16 22:58:13 | INFO     | test_scraper.py:182  | test_scraper_manager() |    Sources: []
2025-06-16 22:58:13 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:58:13 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Change Detection test...
2025-06-16 22:58:13 | INFO     | test_scraper.py:200  | test_change_detection() | ============================================================
2025-06-16 22:58:13 | INFO     | test_scraper.py:201  | test_change_detection() | TESTING CHANGE DETECTION
2025-06-16 22:58:13 | INFO     | test_scraper.py:202  | test_change_detection() | ============================================================
2025-06-16 22:58:13 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:58:13 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 22:58:13 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:58:06.427526
2025-06-16 22:58:13 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:58:13 | INFO     | test_scraper.py:210  | test_change_detection() | First run (should find new content)...
2025-06-16 22:58:13 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:58:13 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:58:13 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:58:13 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: False
2025-06-16 22:58:13 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:58:13 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:58:13 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:58:13 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:58:13 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: False
2025-06-16 22:58:13 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The 'start_year' argument and 'historical_split' argument must be in the past.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2025, verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 48, in __init__
    raise ValueError("The 'start_year' argument and 'historical_split' argument must be in the past.")
ValueError: The 'start_year' argument and 'historical_split' argument must be in the past.
2025-06-16 22:58:13 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:58:13 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:58:06.427526
2025-06-16 22:58:13 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 22:58:13 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: False
2025-06-16 22:58:13 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:28:06.427526
2025-06-16 22:58:13 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:58:15 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:58:16 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:58:17 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:58:18 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:58:18 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:58:19 | INFO     | base_scraper.py:147  | run_scraping() | reddit_wsb: No new content found
2025-06-16 22:58:19 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:58:19 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:58:19 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:58:19 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:58:19 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6764ms
2025-06-16 22:58:19 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 22:58:19 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:58:19 | INFO     | test_scraper.py:214  | test_change_detection() |    First run found: 0 new items
2025-06-16 22:58:19 | INFO     | test_scraper.py:217  | test_change_detection() | Second run (should find little new content)...
2025-06-16 22:58:19 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:58:19 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:58:19 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:58:19 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: False
2025-06-16 22:58:19 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:58:19 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:58:19 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:58:19 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:58:19 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: False
2025-06-16 22:58:19 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The 'start_year' argument and 'historical_split' argument must be in the past.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2025, verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 48, in __init__
    raise ValueError("The 'start_year' argument and 'historical_split' argument must be in the past.")
ValueError: The 'start_year' argument and 'historical_split' argument must be in the past.
2025-06-16 22:58:19 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:58:19 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 22:58:19 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:58:06.427526
2025-06-16 22:58:19 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: False
2025-06-16 22:58:19 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:28:06.427526
2025-06-16 22:58:19 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:58:21 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:58:22 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:58:23 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:58:24 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:58:25 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:58:26 | INFO     | base_scraper.py:147  | run_scraping() | reddit_wsb: No new content found
2025-06-16 22:58:26 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:58:26 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:58:26 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:58:26 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:58:26 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6322ms
2025-06-16 22:58:26 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 22:58:26 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:58:26 | INFO     | test_scraper.py:221  | test_change_detection() |    Second run found: 0 new items
2025-06-16 22:58:26 | INFO     | test_scraper.py:224  | test_change_detection() | Third run with force update (should re-scrape everything)...
2025-06-16 22:58:26 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:58:26 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:58:26 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:58:26 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:58:26 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:58:26 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:58:26 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:58:26 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:58:26 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:58:26 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The 'start_year' argument and 'historical_split' argument must be in the past.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2025, verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 48, in __init__
    raise ValueError("The 'start_year' argument and 'historical_split' argument must be in the past.")
ValueError: The 'start_year' argument and 'historical_split' argument must be in the past.
2025-06-16 22:58:26 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:58:26 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 22:58:26 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:58:06.427526
2025-06-16 22:58:26 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:58:26 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:28:06.427526
2025-06-16 22:58:26 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:58:27 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:58:28 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:58:30 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:58:31 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:58:31 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:58:32 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:58:26.249535
2025-06-16 22:58:32 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:58:32 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:58:32 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:58:32 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:58:32 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:58:32 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:58:32 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6229ms
2025-06-16 22:58:32 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 22:58:32 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:58:32 | INFO     | test_scraper.py:228  | test_change_detection() |    Force update found: 0 new items
2025-06-16 22:58:32 | WARNING  | test_scraper.py:234  | test_change_detection() | ‚ö†Ô∏è  Change detection may not be working optimally
2025-06-16 22:58:32 | INFO     | test_scraper.py:237  | test_change_detection() | ‚úÖ Force update working: Third run re-scraped content
2025-06-16 22:58:32 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:58:32 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Database Cleanup test...
2025-06-16 22:58:32 | INFO     | test_scraper.py:250  | test_database_cleanup() | ============================================================
2025-06-16 22:58:32 | INFO     | test_scraper.py:251  | test_database_cleanup() | TESTING DATABASE CLEANUP
2025-06-16 22:58:32 | INFO     | test_scraper.py:252  | test_database_cleanup() | ============================================================
2025-06-16 22:58:32 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:58:32 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 22:58:32 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:58:26.249535
2025-06-16 22:58:32 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:58:32 | INFO     | test_scraper.py:261  | test_database_cleanup() | Current content items: 0
2025-06-16 22:58:32 | INFO     | test_scraper.py:264  | test_database_cleanup() | Testing cleanup with 0 days (should clean nothing)...
2025-06-16 22:58:32 | INFO     | test_scraper.py:269  | test_database_cleanup() | ‚úÖ Cleanup test successful: 0 records would be deleted
2025-06-16 22:58:32 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:58:32 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Performance Test test...
2025-06-16 22:58:32 | INFO     | test_scraper.py:282  | run_performance_test() | ============================================================
2025-06-16 22:58:32 | INFO     | test_scraper.py:283  | run_performance_test() | RUNNING PERFORMANCE TEST
2025-06-16 22:58:32 | INFO     | test_scraper.py:284  | run_performance_test() | ============================================================
2025-06-16 22:58:32 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:58:32 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 22:58:32 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:58:26.249535
2025-06-16 22:58:32 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:58:32 | INFO     | test_scraper.py:293  | run_performance_test() | Testing sequential execution...
2025-06-16 22:58:32 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:58:32 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:58:32 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:58:32 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:58:32 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: False
2025-06-16 22:58:32 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:58:32 | INFO     | scraper_manager.py:114  | _run_scrapers_sequential() | Running scraper: fed_reserve
2025-06-16 22:58:32 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:58:32 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:58:32 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:58:32 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The 'start_year' argument and 'historical_split' argument must be in the past.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2025, verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 48, in __init__
    raise ValueError("The 'start_year' argument and 'historical_split' argument must be in the past.")
ValueError: The 'start_year' argument and 'historical_split' argument must be in the past.
2025-06-16 22:58:32 | INFO     | scraper_manager.py:125  | _run_scrapers_sequential() | fed_reserve: ‚ùå FAILED
2025-06-16 22:58:32 | INFO     | scraper_manager.py:114  | _run_scrapers_sequential() | Running scraper: reddit_wsb
2025-06-16 22:58:32 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:58:32 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:58:26.249535
2025-06-16 22:58:32 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:58:32 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:28:26.249535
2025-06-16 22:58:32 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:58:34 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:58:35 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:58:36 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:58:37 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:58:37 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:58:38 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:58:32.492225
2025-06-16 22:58:38 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:58:38 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:58:38 | INFO     | scraper_manager.py:125  | _run_scrapers_sequential() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:58:38 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:58:38 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:58:38 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:58:38 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6491ms
2025-06-16 22:58:38 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 22:58:38 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:58:38 | INFO     | test_scraper.py:298  | run_performance_test() | Testing parallel execution...
2025-06-16 22:58:38 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:58:38 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:58:38 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:58:38 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:58:38 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:58:38 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:58:38 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:58:38 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:58:38 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:58:38 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The 'start_year' argument and 'historical_split' argument must be in the past.
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2025, verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 48, in __init__
    raise ValueError("The 'start_year' argument and 'historical_split' argument must be in the past.")
ValueError: The 'start_year' argument and 'historical_split' argument must be in the past.
2025-06-16 22:58:38 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:58:38 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 22:58:38 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:58:32.492225
2025-06-16 22:58:38 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:58:38 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:28:32.492225
2025-06-16 22:58:38 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:58:40 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:58:41 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:58:42 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:58:43 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:58:44 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:58:45 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:58:38.983100
2025-06-16 22:58:45 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:58:45 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:58:45 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:58:45 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:58:45 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:58:45 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:58:45 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6274ms
2025-06-16 22:58:45 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 22:58:45 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:58:45 | INFO     | test_scraper.py:304  | run_performance_test() | Performance comparison:
2025-06-16 22:58:45 | INFO     | test_scraper.py:305  | run_performance_test() |    Sequential: 6.49s
2025-06-16 22:58:45 | INFO     | test_scraper.py:306  | run_performance_test() |    Parallel: 6.27s
2025-06-16 22:58:45 | INFO     | test_scraper.py:310  | run_performance_test() |    ‚úÖ Parallel execution 1.0x faster
2025-06-16 22:58:45 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:58:45 | INFO     | test_scraper.py:350  | main() | 
================================================================================
2025-06-16 22:58:45 | INFO     | test_scraper.py:351  | main() | SCRAPER SYSTEM TEST SUMMARY
2025-06-16 22:58:45 | INFO     | test_scraper.py:352  | main() | ================================================================================
2025-06-16 22:58:45 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Fed Scraper
2025-06-16 22:58:45 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Reddit WSB Scraper
2025-06-16 22:58:45 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Scraper Manager
2025-06-16 22:58:45 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Change Detection
2025-06-16 22:58:45 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Database Cleanup
2025-06-16 22:58:45 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Performance Test
2025-06-16 22:58:45 | INFO     | test_scraper.py:361  | main() | --------------------------------------------------------------------------------
2025-06-16 22:58:45 | INFO     | test_scraper.py:362  | main() | OVERALL RESULT: 5/6 tests passed
2025-06-16 22:58:45 | ERROR    | test_scraper.py:368  | main() | ‚ö†Ô∏è  1 tests failed. Check the logs above for details.
2025-06-16 22:58:57 | INFO     | logging_config.py:93   | initialize() | ============================================================
2025-06-16 22:58:57 | INFO     | logging_config.py:94   | initialize() | UNIFIED SCREENER SYSTEM STARTING
2025-06-16 22:58:57 | INFO     | logging_config.py:95   | initialize() | Log Level: INFO
2025-06-16 22:58:57 | INFO     | logging_config.py:96   | initialize() | Log File: logs\scraper_test.log
2025-06-16 22:58:57 | INFO     | logging_config.py:97   | initialize() | Console Output: True
2025-06-16 22:58:57 | INFO     | logging_config.py:98   | initialize() | All modules will log to this unified logger
2025-06-16 22:58:57 | INFO     | logging_config.py:99   | initialize() | ============================================================
2025-06-16 22:58:57 | INFO     | test_scraper.py:324  | main() | üß™ STARTING MULTI-SCRAPER SYSTEM TESTS
2025-06-16 22:58:57 | INFO     | test_scraper.py:325  | main() | ================================================================================
2025-06-16 22:58:57 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Fed Scraper test...
2025-06-16 22:58:57 | INFO     | test_scraper.py:29   | test_fed_scraper() | ============================================================
2025-06-16 22:58:57 | INFO     | test_scraper.py:30   | test_fed_scraper() | TESTING FEDERAL RESERVE SCRAPER
2025-06-16 22:58:57 | INFO     | test_scraper.py:31   | test_fed_scraper() | ============================================================
2025-06-16 22:59:04 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:59:04 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 22:59:04 | INFO     | test_scraper.py:42   | test_fed_scraper() | ‚úÖ Fed scraper initialized
2025-06-16 22:59:04 | INFO     | test_scraper.py:45   | test_fed_scraper() | Running Fed scraper...
2025-06-16 22:59:04 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:59:04 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:59:04 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:59:04 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2024,  verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 50, in __init__
    raise Warning("The historical split value is typically after the start of the period. "
                  "Please consult the Federal Reserve Website")
Warning: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
2025-06-16 22:59:04 | ERROR    | test_scraper.py:61   | test_fed_scraper() | ‚ùå Fed scraper FAILED: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
2025-06-16 22:59:04 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 22:59:04 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Reddit WSB Scraper test...
2025-06-16 22:59:04 | INFO     | test_scraper.py:72   | test_reddit_scraper() | ============================================================
2025-06-16 22:59:04 | INFO     | test_scraper.py:73   | test_reddit_scraper() | TESTING REDDIT WSB SCRAPER
2025-06-16 22:59:04 | INFO     | test_scraper.py:74   | test_reddit_scraper() | ============================================================
2025-06-16 22:59:04 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:59:04 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:58:38.983100
2025-06-16 22:59:04 | INFO     | test_scraper.py:85   | test_reddit_scraper() | ‚úÖ Reddit WSB scraper initialized
2025-06-16 22:59:04 | INFO     | test_scraper.py:88   | test_reddit_scraper() | Running Reddit WSB scraper...
2025-06-16 22:59:04 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:59:04 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:58:38.983100
2025-06-16 22:59:04 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:59:04 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:28:38.983100
2025-06-16 22:59:04 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:59:06 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:59:07 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:59:08 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:59:09 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:59:09 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:59:10 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:59:04.450171
2025-06-16 22:59:10 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:59:10 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:59:10 | INFO     | test_scraper.py:92   | test_reddit_scraper() | ‚úÖ Reddit WSB scraper SUCCESS
2025-06-16 22:59:10 | INFO     | test_scraper.py:93   | test_reddit_scraper() |    New content: 0
2025-06-16 22:59:10 | INFO     | test_scraper.py:94   | test_reddit_scraper() |    Total found: 0
2025-06-16 22:59:10 | INFO     | test_scraper.py:95   | test_reddit_scraper() |    Execution time: 6464ms
2025-06-16 22:59:10 | INFO     | test_scraper.py:96   | test_reddit_scraper() |    Message: Found 0 items, saved 0 new
2025-06-16 22:59:10 | INFO     | test_scraper.py:99   | test_reddit_scraper() | Getting trending tickers...
2025-06-16 22:59:12 | INFO     | test_scraper.py:110  | test_reddit_scraper() |    No trending tickers found
2025-06-16 22:59:12 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:59:12 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Scraper Manager test...
2025-06-16 22:59:12 | INFO     | test_scraper.py:124  | test_scraper_manager() | ============================================================
2025-06-16 22:59:12 | INFO     | test_scraper.py:125  | test_scraper_manager() | TESTING SCRAPER MANAGER
2025-06-16 22:59:12 | INFO     | test_scraper.py:126  | test_scraper_manager() | ============================================================
2025-06-16 22:59:12 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:59:12 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 22:59:12 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:59:04.450171
2025-06-16 22:59:12 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:59:12 | INFO     | test_scraper.py:133  | test_scraper_manager() | ‚úÖ Scraper manager initialized
2025-06-16 22:59:12 | INFO     | test_scraper.py:136  | test_scraper_manager() | Getting scraper status...
2025-06-16 22:59:12 | INFO     | test_scraper.py:138  | test_scraper_manager() |    Total scrapers: 2
2025-06-16 22:59:12 | INFO     | test_scraper.py:143  | test_scraper_manager() |    fed_reserve: Last run: 2025-06-16T22:55:19.283048, Success rate: 0.0%
2025-06-16 22:59:12 | INFO     | test_scraper.py:143  | test_scraper_manager() |    reddit_wsb: Last run: 2025-06-16T22:59:04.450171, Success rate: 0.0%
2025-06-16 22:59:12 | INFO     | test_scraper.py:146  | test_scraper_manager() | Running all scrapers in parallel...
2025-06-16 22:59:12 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:59:12 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:59:12 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:59:12 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:59:12 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:59:12 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:59:12 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:59:12 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:59:12 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:59:12 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2024,  verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 50, in __init__
    raise Warning("The historical split value is typically after the start of the period. "
                  "Please consult the Federal Reserve Website")
Warning: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
2025-06-16 22:59:12 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:59:12 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:59:04.450171
2025-06-16 22:59:12 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 22:59:12 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:59:12 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:29:04.450171
2025-06-16 22:59:12 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:59:14 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:59:15 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:59:16 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:59:17 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:59:17 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:59:18 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:59:12.601768
2025-06-16 22:59:18 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:59:18 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:59:18 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:59:18 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:59:18 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:59:18 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:59:18 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6385ms
2025-06-16 22:59:18 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 22:59:18 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:59:18 | INFO     | test_scraper.py:151  | test_scraper_manager() | ‚úÖ Multi-scraper execution SUCCESS
2025-06-16 22:59:18 | INFO     | test_scraper.py:152  | test_scraper_manager() |    Successful scrapers: 1/2
2025-06-16 22:59:18 | INFO     | test_scraper.py:153  | test_scraper_manager() |    Total new content: 0
2025-06-16 22:59:18 | INFO     | test_scraper.py:154  | test_scraper_manager() |    Total execution time: 6385ms
2025-06-16 22:59:18 | INFO     | test_scraper.py:157  | test_scraper_manager() |    Per-scraper results:
2025-06-16 22:59:18 | INFO     | test_scraper.py:162  | test_scraper_manager() |      ‚ùå fed_reserve: 0 new items (2ms)
2025-06-16 22:59:18 | INFO     | test_scraper.py:162  | test_scraper_manager() |      ‚úÖ reddit_wsb: 0 new items (6382ms)
2025-06-16 22:59:18 | INFO     | test_scraper.py:167  | test_scraper_manager() | Getting recent content...
2025-06-16 22:59:18 | INFO     | test_scraper.py:169  | test_scraper_manager() |    Found 0 recent items
2025-06-16 22:59:18 | INFO     | test_scraper.py:177  | test_scraper_manager() | Getting trending analysis...
2025-06-16 22:59:18 | INFO     | test_scraper.py:181  | test_scraper_manager() |    Content items analyzed: 0
2025-06-16 22:59:18 | INFO     | test_scraper.py:182  | test_scraper_manager() |    Sources: []
2025-06-16 22:59:18 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:59:18 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Change Detection test...
2025-06-16 22:59:18 | INFO     | test_scraper.py:200  | test_change_detection() | ============================================================
2025-06-16 22:59:18 | INFO     | test_scraper.py:201  | test_change_detection() | TESTING CHANGE DETECTION
2025-06-16 22:59:18 | INFO     | test_scraper.py:202  | test_change_detection() | ============================================================
2025-06-16 22:59:18 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:59:18 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 22:59:18 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:59:12.601768
2025-06-16 22:59:18 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:59:18 | INFO     | test_scraper.py:210  | test_change_detection() | First run (should find new content)...
2025-06-16 22:59:18 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:59:18 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:59:18 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:59:18 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: False
2025-06-16 22:59:18 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:59:18 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:59:18 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:59:18 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:59:18 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: False
2025-06-16 22:59:18 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:59:18 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2024,  verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 50, in __init__
    raise Warning("The historical split value is typically after the start of the period. "
                  "Please consult the Federal Reserve Website")
Warning: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
2025-06-16 22:59:18 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:59:12.601768
2025-06-16 22:59:18 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 22:59:18 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: False
2025-06-16 22:59:18 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:29:12.601768
2025-06-16 22:59:18 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:59:21 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:59:22 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:59:23 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:59:24 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:59:24 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:59:25 | INFO     | base_scraper.py:147  | run_scraping() | reddit_wsb: No new content found
2025-06-16 22:59:25 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:59:25 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:59:25 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:59:25 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:59:25 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6692ms
2025-06-16 22:59:25 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 22:59:25 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:59:25 | INFO     | test_scraper.py:214  | test_change_detection() |    First run found: 0 new items
2025-06-16 22:59:25 | INFO     | test_scraper.py:217  | test_change_detection() | Second run (should find little new content)...
2025-06-16 22:59:25 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:59:25 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:59:25 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:59:25 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: False
2025-06-16 22:59:25 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:59:25 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:59:25 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:59:25 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:59:25 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:59:25 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: False
2025-06-16 22:59:25 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:59:12.601768
2025-06-16 22:59:25 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2024,  verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 50, in __init__
    raise Warning("The historical split value is typically after the start of the period. "
                  "Please consult the Federal Reserve Website")
Warning: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
2025-06-16 22:59:25 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: False
2025-06-16 22:59:25 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 22:59:25 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:29:12.601768
2025-06-16 22:59:25 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:59:27 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:59:28 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:59:29 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:59:30 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:59:31 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:59:32 | INFO     | base_scraper.py:147  | run_scraping() | reddit_wsb: No new content found
2025-06-16 22:59:32 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:59:32 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:59:32 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:59:32 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:59:32 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 6594ms
2025-06-16 22:59:32 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 22:59:32 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:59:32 | INFO     | test_scraper.py:221  | test_change_detection() |    Second run found: 0 new items
2025-06-16 22:59:32 | INFO     | test_scraper.py:224  | test_change_detection() | Third run with force update (should re-scrape everything)...
2025-06-16 22:59:32 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:59:32 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:59:32 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:59:32 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:59:32 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 22:59:32 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:59:32 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:59:32 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:59:32 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:59:32 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:59:32 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:59:12.601768
2025-06-16 22:59:32 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2024,  verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 50, in __init__
    raise Warning("The historical split value is typically after the start of the period. "
                  "Please consult the Federal Reserve Website")
Warning: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
2025-06-16 22:59:32 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:59:32 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:29:12.601768
2025-06-16 22:59:32 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 22:59:32 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:59:34 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 22:59:35 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 22:59:37 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 22:59:38 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 22:59:38 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 22:59:39 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 22:59:32.285036
2025-06-16 22:59:39 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 22:59:39 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 22:59:39 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 22:59:39 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 22:59:39 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 22:59:39 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 22:59:39 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 7278ms
2025-06-16 22:59:39 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 22:59:39 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 22:59:39 | INFO     | test_scraper.py:228  | test_change_detection() |    Force update found: 0 new items
2025-06-16 22:59:39 | WARNING  | test_scraper.py:234  | test_change_detection() | ‚ö†Ô∏è  Change detection may not be working optimally
2025-06-16 22:59:39 | INFO     | test_scraper.py:237  | test_change_detection() | ‚úÖ Force update working: Third run re-scraped content
2025-06-16 22:59:39 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:59:39 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Database Cleanup test...
2025-06-16 22:59:39 | INFO     | test_scraper.py:250  | test_database_cleanup() | ============================================================
2025-06-16 22:59:39 | INFO     | test_scraper.py:251  | test_database_cleanup() | TESTING DATABASE CLEANUP
2025-06-16 22:59:39 | INFO     | test_scraper.py:252  | test_database_cleanup() | ============================================================
2025-06-16 22:59:39 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:59:39 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 22:59:39 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:59:32.285036
2025-06-16 22:59:39 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:59:39 | INFO     | test_scraper.py:261  | test_database_cleanup() | Current content items: 0
2025-06-16 22:59:39 | INFO     | test_scraper.py:264  | test_database_cleanup() | Testing cleanup with 0 days (should clean nothing)...
2025-06-16 22:59:39 | INFO     | test_scraper.py:269  | test_database_cleanup() | ‚úÖ Cleanup test successful: 0 records would be deleted
2025-06-16 22:59:39 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 22:59:39 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Performance Test test...
2025-06-16 22:59:39 | INFO     | test_scraper.py:282  | run_performance_test() | ============================================================
2025-06-16 22:59:39 | INFO     | test_scraper.py:283  | run_performance_test() | RUNNING PERFORMANCE TEST
2025-06-16 22:59:39 | INFO     | test_scraper.py:284  | run_performance_test() | ============================================================
2025-06-16 22:59:39 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:59:39 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 22:59:39 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:59:32.285036
2025-06-16 22:59:39 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 22:59:39 | INFO     | test_scraper.py:293  | run_performance_test() | Testing sequential execution...
2025-06-16 22:59:39 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 22:59:39 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 22:59:39 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 22:59:39 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 22:59:39 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: False
2025-06-16 22:59:39 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 22:59:39 | INFO     | scraper_manager.py:114  | _run_scrapers_sequential() | Running scraper: fed_reserve
2025-06-16 22:59:39 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:59:39 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:59:39 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 22:59:39 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 27, in scrape_new_content
    mins_df = FederalReserveMins(historical_split=2014, start_year=2024,  verbose=True).find_minutes()
              ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\FedMins.py", line 50, in __init__
    raise Warning("The historical split value is typically after the start of the period. "
                  "Please consult the Federal Reserve Website")
Warning: The historical split value is typically after the start of the period. Please consult the Federal Reserve Website
2025-06-16 22:59:39 | INFO     | scraper_manager.py:125  | _run_scrapers_sequential() | fed_reserve: ‚ùå FAILED
2025-06-16 22:59:39 | INFO     | scraper_manager.py:114  | _run_scrapers_sequential() | Running scraper: reddit_wsb
2025-06-16 22:59:39 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 22:59:39 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:59:32.285036
2025-06-16 22:59:39 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 22:59:39 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:29:32.285036
2025-06-16 22:59:39 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 22:59:43 | INFO     | logging_config.py:93   | initialize() | ============================================================
2025-06-16 22:59:43 | INFO     | logging_config.py:94   | initialize() | UNIFIED SCREENER SYSTEM STARTING
2025-06-16 22:59:43 | INFO     | logging_config.py:95   | initialize() | Log Level: INFO
2025-06-16 22:59:43 | INFO     | logging_config.py:96   | initialize() | Log File: logs\scraper_test.log
2025-06-16 22:59:43 | INFO     | logging_config.py:97   | initialize() | Console Output: True
2025-06-16 22:59:43 | INFO     | logging_config.py:98   | initialize() | All modules will log to this unified logger
2025-06-16 22:59:43 | INFO     | logging_config.py:99   | initialize() | ============================================================
2025-06-16 22:59:43 | INFO     | test_scraper.py:324  | main() | üß™ STARTING MULTI-SCRAPER SYSTEM TESTS
2025-06-16 22:59:43 | INFO     | test_scraper.py:325  | main() | ================================================================================
2025-06-16 22:59:43 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Fed Scraper test...
2025-06-16 22:59:43 | INFO     | test_scraper.py:29   | test_fed_scraper() | ============================================================
2025-06-16 22:59:43 | INFO     | test_scraper.py:30   | test_fed_scraper() | TESTING FEDERAL RESERVE SCRAPER
2025-06-16 22:59:43 | INFO     | test_scraper.py:31   | test_fed_scraper() | ============================================================
2025-06-16 22:59:50 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 22:59:50 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 22:59:50 | INFO     | test_scraper.py:42   | test_fed_scraper() | ‚úÖ Fed scraper initialized
2025-06-16 22:59:50 | INFO     | test_scraper.py:45   | test_fed_scraper() | Running Fed scraper...
2025-06-16 22:59:50 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 22:59:50 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 22:59:50 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 23:00:07 | INFO     | logging_config.py:93   | initialize() | ============================================================
2025-06-16 23:00:07 | INFO     | logging_config.py:94   | initialize() | UNIFIED SCREENER SYSTEM STARTING
2025-06-16 23:00:07 | INFO     | logging_config.py:95   | initialize() | Log Level: INFO
2025-06-16 23:00:07 | INFO     | logging_config.py:96   | initialize() | Log File: logs\scraper_test.log
2025-06-16 23:00:07 | INFO     | logging_config.py:97   | initialize() | Console Output: True
2025-06-16 23:00:07 | INFO     | logging_config.py:98   | initialize() | All modules will log to this unified logger
2025-06-16 23:00:07 | INFO     | logging_config.py:99   | initialize() | ============================================================
2025-06-16 23:00:07 | INFO     | test_scraper.py:324  | main() | üß™ STARTING MULTI-SCRAPER SYSTEM TESTS
2025-06-16 23:00:07 | INFO     | test_scraper.py:325  | main() | ================================================================================
2025-06-16 23:00:07 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Fed Scraper test...
2025-06-16 23:00:07 | INFO     | test_scraper.py:29   | test_fed_scraper() | ============================================================
2025-06-16 23:00:07 | INFO     | test_scraper.py:30   | test_fed_scraper() | TESTING FEDERAL RESERVE SCRAPER
2025-06-16 23:00:07 | INFO     | test_scraper.py:31   | test_fed_scraper() | ============================================================
2025-06-16 23:00:14 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 23:00:14 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 23:00:14 | INFO     | test_scraper.py:42   | test_fed_scraper() | ‚úÖ Fed scraper initialized
2025-06-16 23:00:14 | INFO     | test_scraper.py:45   | test_fed_scraper() | Running Fed scraper...
2025-06-16 23:00:14 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 23:00:14 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 23:00:14 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 23:00:40 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: Length mismatch: Expected axis has 0 elements, new values have 1 elements
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 28, in scrape_new_content
    bb_df = BeigeBooks(historical_split=2019, verbose=True).find_beige_books()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\Beigebook.py", line 204, in find_beige_books
    self.dataset.columns = ['Beige_Book']
    ^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 6332, in __setattr__
    return object.__setattr__(self, name, value)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^
  File "pandas/_libs/properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 814, in _set_axis
    self._mgr.set_axis(axis, labels)
    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\managers.py", line 238, in set_axis
    self._validate_set_axis(axis, new_labels)
    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\base.py", line 98, in _validate_set_axis
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Length mismatch: Expected axis has 0 elements, new values have 1 elements
2025-06-16 23:00:40 | ERROR    | test_scraper.py:61   | test_fed_scraper() | ‚ùå Fed scraper FAILED: Length mismatch: Expected axis has 0 elements, new values have 1 elements
2025-06-16 23:00:40 | INFO     | test_scraper.py:344  | main() |    ‚ùå FAILED
2025-06-16 23:00:40 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Reddit WSB Scraper test...
2025-06-16 23:00:40 | INFO     | test_scraper.py:72   | test_reddit_scraper() | ============================================================
2025-06-16 23:00:40 | INFO     | test_scraper.py:73   | test_reddit_scraper() | TESTING REDDIT WSB SCRAPER
2025-06-16 23:00:40 | INFO     | test_scraper.py:74   | test_reddit_scraper() | ============================================================
2025-06-16 23:00:40 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 23:00:40 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 22:59:32.285036
2025-06-16 23:00:40 | INFO     | test_scraper.py:85   | test_reddit_scraper() | ‚úÖ Reddit WSB scraper initialized
2025-06-16 23:00:40 | INFO     | test_scraper.py:88   | test_reddit_scraper() | Running Reddit WSB scraper...
2025-06-16 23:00:40 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 23:00:40 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 22:59:32.285036
2025-06-16 23:00:40 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 23:00:40 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:29:32.285036
2025-06-16 23:00:40 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 23:00:43 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 23:00:44 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 23:00:45 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 23:00:46 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 23:00:46 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 23:00:47 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 23:00:40.759598
2025-06-16 23:00:47 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 23:00:47 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 23:00:47 | INFO     | test_scraper.py:92   | test_reddit_scraper() | ‚úÖ Reddit WSB scraper SUCCESS
2025-06-16 23:00:47 | INFO     | test_scraper.py:93   | test_reddit_scraper() |    New content: 0
2025-06-16 23:00:47 | INFO     | test_scraper.py:94   | test_reddit_scraper() |    Total found: 0
2025-06-16 23:00:47 | INFO     | test_scraper.py:95   | test_reddit_scraper() |    Execution time: 7173ms
2025-06-16 23:00:47 | INFO     | test_scraper.py:96   | test_reddit_scraper() |    Message: Found 0 items, saved 0 new
2025-06-16 23:00:47 | INFO     | test_scraper.py:99   | test_reddit_scraper() | Getting trending tickers...
2025-06-16 23:00:49 | INFO     | test_scraper.py:110  | test_reddit_scraper() |    No trending tickers found
2025-06-16 23:00:49 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 23:00:49 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Scraper Manager test...
2025-06-16 23:00:49 | INFO     | test_scraper.py:124  | test_scraper_manager() | ============================================================
2025-06-16 23:00:49 | INFO     | test_scraper.py:125  | test_scraper_manager() | TESTING SCRAPER MANAGER
2025-06-16 23:00:49 | INFO     | test_scraper.py:126  | test_scraper_manager() | ============================================================
2025-06-16 23:00:49 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 23:00:49 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 23:00:49 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 23:00:40.759598
2025-06-16 23:00:49 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 23:00:49 | INFO     | test_scraper.py:133  | test_scraper_manager() | ‚úÖ Scraper manager initialized
2025-06-16 23:00:49 | INFO     | test_scraper.py:136  | test_scraper_manager() | Getting scraper status...
2025-06-16 23:00:49 | INFO     | test_scraper.py:138  | test_scraper_manager() |    Total scrapers: 2
2025-06-16 23:00:49 | INFO     | test_scraper.py:143  | test_scraper_manager() |    fed_reserve: Last run: 2025-06-16T22:55:19.283048, Success rate: 0.0%
2025-06-16 23:00:49 | INFO     | test_scraper.py:143  | test_scraper_manager() |    reddit_wsb: Last run: 2025-06-16T23:00:40.759598, Success rate: 0.0%
2025-06-16 23:00:49 | INFO     | test_scraper.py:146  | test_scraper_manager() | Running all scrapers in parallel...
2025-06-16 23:00:49 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 23:00:49 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 23:00:49 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 23:00:49 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 23:00:49 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 23:00:49 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 23:00:49 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 23:00:49 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 23:00:49 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 23:00:49 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 23:00:49 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 23:00:40.759598
2025-06-16 23:00:49 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 23:00:49 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:30:40.759598
2025-06-16 23:00:49 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 23:00:52 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 23:00:53 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 23:00:54 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 23:00:55 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 23:00:56 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 23:00:57 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 23:00:49.852463
2025-06-16 23:00:57 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 23:00:57 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 23:00:57 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 23:01:26 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: Length mismatch: Expected axis has 0 elements, new values have 1 elements
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 28, in scrape_new_content
    bb_df = BeigeBooks(historical_split=2019, verbose=True).find_beige_books()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\Beigebook.py", line 204, in find_beige_books
    self.dataset.columns = ['Beige_Book']
    ^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 6332, in __setattr__
    return object.__setattr__(self, name, value)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^
  File "pandas/_libs/properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 814, in _set_axis
    self._mgr.set_axis(axis, labels)
    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\managers.py", line 238, in set_axis
    self._validate_set_axis(axis, new_labels)
    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\base.py", line 98, in _validate_set_axis
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Length mismatch: Expected axis has 0 elements, new values have 1 elements
2025-06-16 23:01:26 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 23:01:26 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 23:01:26 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 23:01:26 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 23:01:26 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 36575ms
2025-06-16 23:01:26 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 23:01:26 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 23:01:26 | INFO     | test_scraper.py:151  | test_scraper_manager() | ‚úÖ Multi-scraper execution SUCCESS
2025-06-16 23:01:26 | INFO     | test_scraper.py:152  | test_scraper_manager() |    Successful scrapers: 1/2
2025-06-16 23:01:26 | INFO     | test_scraper.py:153  | test_scraper_manager() |    Total new content: 0
2025-06-16 23:01:26 | INFO     | test_scraper.py:154  | test_scraper_manager() |    Total execution time: 36575ms
2025-06-16 23:01:26 | INFO     | test_scraper.py:157  | test_scraper_manager() |    Per-scraper results:
2025-06-16 23:01:26 | INFO     | test_scraper.py:162  | test_scraper_manager() |      ‚úÖ reddit_wsb: 0 new items (8107ms)
2025-06-16 23:01:26 | INFO     | test_scraper.py:162  | test_scraper_manager() |      ‚ùå fed_reserve: 0 new items (36567ms)
2025-06-16 23:01:26 | INFO     | test_scraper.py:167  | test_scraper_manager() | Getting recent content...
2025-06-16 23:01:26 | INFO     | test_scraper.py:169  | test_scraper_manager() |    Found 0 recent items
2025-06-16 23:01:26 | INFO     | test_scraper.py:177  | test_scraper_manager() | Getting trending analysis...
2025-06-16 23:01:26 | INFO     | test_scraper.py:181  | test_scraper_manager() |    Content items analyzed: 0
2025-06-16 23:01:26 | INFO     | test_scraper.py:182  | test_scraper_manager() |    Sources: []
2025-06-16 23:01:26 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 23:01:26 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Change Detection test...
2025-06-16 23:01:26 | INFO     | test_scraper.py:200  | test_change_detection() | ============================================================
2025-06-16 23:01:26 | INFO     | test_scraper.py:201  | test_change_detection() | TESTING CHANGE DETECTION
2025-06-16 23:01:26 | INFO     | test_scraper.py:202  | test_change_detection() | ============================================================
2025-06-16 23:01:26 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 23:01:26 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 23:01:26 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 23:00:49.852463
2025-06-16 23:01:26 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 23:01:26 | INFO     | test_scraper.py:210  | test_change_detection() | First run (should find new content)...
2025-06-16 23:01:26 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 23:01:26 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 23:01:26 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 23:01:26 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: False
2025-06-16 23:01:26 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 23:01:26 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 23:01:26 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 23:01:26 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 23:01:26 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: False
2025-06-16 23:01:26 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 23:01:26 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 23:00:49.852463
2025-06-16 23:01:26 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: False
2025-06-16 23:01:26 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:30:49.852463
2025-06-16 23:01:26 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 23:01:28 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 23:01:29 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 23:01:30 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 23:01:31 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 23:01:32 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 23:01:33 | INFO     | base_scraper.py:147  | run_scraping() | reddit_wsb: No new content found
2025-06-16 23:01:33 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 23:01:59 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: Length mismatch: Expected axis has 0 elements, new values have 1 elements
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 28, in scrape_new_content
    bb_df = BeigeBooks(historical_split=2019, verbose=True).find_beige_books()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\Beigebook.py", line 204, in find_beige_books
    self.dataset.columns = ['Beige_Book']
    ^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 6332, in __setattr__
    return object.__setattr__(self, name, value)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^
  File "pandas/_libs/properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 814, in _set_axis
    self._mgr.set_axis(axis, labels)
    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\managers.py", line 238, in set_axis
    self._validate_set_axis(axis, new_labels)
    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\base.py", line 98, in _validate_set_axis
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Length mismatch: Expected axis has 0 elements, new values have 1 elements
2025-06-16 23:01:59 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 23:01:59 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 23:01:59 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 23:01:59 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 23:01:59 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 33146ms
2025-06-16 23:01:59 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 23:01:59 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 23:01:59 | INFO     | test_scraper.py:214  | test_change_detection() |    First run found: 0 new items
2025-06-16 23:01:59 | INFO     | test_scraper.py:217  | test_change_detection() | Second run (should find little new content)...
2025-06-16 23:01:59 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 23:01:59 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 23:01:59 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 23:01:59 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: False
2025-06-16 23:01:59 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 23:01:59 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 23:01:59 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 23:01:59 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 23:01:59 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: False
2025-06-16 23:01:59 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 23:01:59 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 23:00:49.852463
2025-06-16 23:01:59 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: False
2025-06-16 23:01:59 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:30:49.852463
2025-06-16 23:01:59 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 23:02:01 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 23:02:02 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 23:02:03 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 23:02:04 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 23:02:05 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 23:02:06 | INFO     | base_scraper.py:147  | run_scraping() | reddit_wsb: No new content found
2025-06-16 23:02:06 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 23:02:25 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: Length mismatch: Expected axis has 0 elements, new values have 1 elements
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 28, in scrape_new_content
    bb_df = BeigeBooks(historical_split=2019, verbose=True).find_beige_books()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\Beigebook.py", line 204, in find_beige_books
    self.dataset.columns = ['Beige_Book']
    ^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 6332, in __setattr__
    return object.__setattr__(self, name, value)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^
  File "pandas/_libs/properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 814, in _set_axis
    self._mgr.set_axis(axis, labels)
    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\managers.py", line 238, in set_axis
    self._validate_set_axis(axis, new_labels)
    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\base.py", line 98, in _validate_set_axis
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Length mismatch: Expected axis has 0 elements, new values have 1 elements
2025-06-16 23:02:25 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 23:02:25 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 23:02:25 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 23:02:25 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 23:02:25 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 25677ms
2025-06-16 23:02:25 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 23:02:25 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 23:02:25 | INFO     | test_scraper.py:221  | test_change_detection() |    Second run found: 0 new items
2025-06-16 23:02:25 | INFO     | test_scraper.py:224  | test_change_detection() | Third run with force update (should re-scrape everything)...
2025-06-16 23:02:25 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 23:02:25 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 23:02:25 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 23:02:25 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 23:02:25 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 23:02:25 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 23:02:25 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 23:02:25 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 23:02:25 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 23:02:25 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 23:02:25 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 23:00:49.852463
2025-06-16 23:02:25 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 23:02:25 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:30:49.852463
2025-06-16 23:02:25 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 23:02:27 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 23:02:28 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 23:02:29 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 23:02:30 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 23:02:30 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 23:02:31 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 23:02:25.260120
2025-06-16 23:02:31 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 23:02:31 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 23:02:31 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 23:02:50 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: Length mismatch: Expected axis has 0 elements, new values have 1 elements
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 28, in scrape_new_content
    bb_df = BeigeBooks(historical_split=2019, verbose=True).find_beige_books()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\Beigebook.py", line 204, in find_beige_books
    self.dataset.columns = ['Beige_Book']
    ^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 6332, in __setattr__
    return object.__setattr__(self, name, value)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^
  File "pandas/_libs/properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 814, in _set_axis
    self._mgr.set_axis(axis, labels)
    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\managers.py", line 238, in set_axis
    self._validate_set_axis(axis, new_labels)
    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\base.py", line 98, in _validate_set_axis
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Length mismatch: Expected axis has 0 elements, new values have 1 elements
2025-06-16 23:02:50 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 23:02:50 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 23:02:50 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 23:02:50 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 23:02:50 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 25616ms
2025-06-16 23:02:50 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 23:02:50 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 23:02:50 | INFO     | test_scraper.py:228  | test_change_detection() |    Force update found: 0 new items
2025-06-16 23:02:50 | WARNING  | test_scraper.py:234  | test_change_detection() | ‚ö†Ô∏è  Change detection may not be working optimally
2025-06-16 23:02:50 | INFO     | test_scraper.py:237  | test_change_detection() | ‚úÖ Force update working: Third run re-scraped content
2025-06-16 23:02:50 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 23:02:50 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Database Cleanup test...
2025-06-16 23:02:50 | INFO     | test_scraper.py:250  | test_database_cleanup() | ============================================================
2025-06-16 23:02:50 | INFO     | test_scraper.py:251  | test_database_cleanup() | TESTING DATABASE CLEANUP
2025-06-16 23:02:50 | INFO     | test_scraper.py:252  | test_database_cleanup() | ============================================================
2025-06-16 23:02:50 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 23:02:50 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 23:02:50 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 23:02:25.260120
2025-06-16 23:02:50 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 23:02:50 | INFO     | test_scraper.py:261  | test_database_cleanup() | Current content items: 0
2025-06-16 23:02:50 | INFO     | test_scraper.py:264  | test_database_cleanup() | Testing cleanup with 0 days (should clean nothing)...
2025-06-16 23:02:50 | INFO     | test_scraper.py:269  | test_database_cleanup() | ‚úÖ Cleanup test successful: 0 records would be deleted
2025-06-16 23:02:50 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 23:02:50 | INFO     | test_scraper.py:339  | main() | 
üß™ Running Performance Test test...
2025-06-16 23:02:50 | INFO     | test_scraper.py:282  | run_performance_test() | ============================================================
2025-06-16 23:02:50 | INFO     | test_scraper.py:283  | run_performance_test() | RUNNING PERFORMANCE TEST
2025-06-16 23:02:50 | INFO     | test_scraper.py:284  | run_performance_test() | ============================================================
2025-06-16 23:02:50 | INFO     | database.py:24   | create_tables() | Database tables created successfully
2025-06-16 23:02:50 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | fed_reserve: Last check time loaded: 2025-06-16 22:55:19.283048
2025-06-16 23:02:50 | DEBUG    | base_scraper.py:44   | _load_last_check_time() | reddit_wsb: Last check time loaded: 2025-06-16 23:02:25.260120
2025-06-16 23:02:50 | INFO     | scraper_manager.py:32   | __init__() | ScraperManager initialized with 2 scrapers
2025-06-16 23:02:50 | INFO     | test_scraper.py:293  | run_performance_test() | Testing sequential execution...
2025-06-16 23:02:50 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 23:02:50 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 23:02:50 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 23:02:50 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 23:02:50 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: False
2025-06-16 23:02:50 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 23:02:50 | INFO     | scraper_manager.py:114  | _run_scrapers_sequential() | Running scraper: fed_reserve
2025-06-16 23:02:50 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 23:02:50 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 23:02:50 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 23:03:17 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: Length mismatch: Expected axis has 0 elements, new values have 1 elements
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 28, in scrape_new_content
    bb_df = BeigeBooks(historical_split=2019, verbose=True).find_beige_books()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\Beigebook.py", line 204, in find_beige_books
    self.dataset.columns = ['Beige_Book']
    ^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 6332, in __setattr__
    return object.__setattr__(self, name, value)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^
  File "pandas/_libs/properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 814, in _set_axis
    self._mgr.set_axis(axis, labels)
    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\managers.py", line 238, in set_axis
    self._validate_set_axis(axis, new_labels)
    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\base.py", line 98, in _validate_set_axis
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Length mismatch: Expected axis has 0 elements, new values have 1 elements
2025-06-16 23:03:17 | INFO     | scraper_manager.py:125  | _run_scrapers_sequential() | fed_reserve: ‚ùå FAILED
2025-06-16 23:03:17 | INFO     | scraper_manager.py:114  | _run_scrapers_sequential() | Running scraper: reddit_wsb
2025-06-16 23:03:17 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 23:03:17 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 23:02:25.260120
2025-06-16 23:03:17 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 23:03:17 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:32:25.260120
2025-06-16 23:03:17 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 23:03:19 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 23:03:20 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 23:03:21 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 23:03:22 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 23:03:23 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 23:03:24 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 23:03:17.107358
2025-06-16 23:03:24 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 23:03:24 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 23:03:24 | INFO     | scraper_manager.py:125  | _run_scrapers_sequential() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 23:03:24 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 23:03:24 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 23:03:24 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 23:03:24 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 33312ms
2025-06-16 23:03:24 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 23:03:24 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 23:03:24 | INFO     | test_scraper.py:298  | run_performance_test() | Testing parallel execution...
2025-06-16 23:03:24 | INFO     | scraper_manager.py:41   | run_all_scrapers() | ============================================================
2025-06-16 23:03:24 | INFO     | scraper_manager.py:42   | run_all_scrapers() | STARTING MULTI-SCRAPER EXECUTION
2025-06-16 23:03:24 | INFO     | scraper_manager.py:43   | run_all_scrapers() | ============================================================
2025-06-16 23:03:24 | INFO     | scraper_manager.py:44   | run_all_scrapers() | Force update: True
2025-06-16 23:03:24 | INFO     | scraper_manager.py:45   | run_all_scrapers() | Parallel execution: True
2025-06-16 23:03:24 | INFO     | scraper_manager.py:46   | run_all_scrapers() | Timeout: 300s
2025-06-16 23:03:24 | INFO     | base_scraper.py:138  | run_scraping() | fed_reserve: Starting scraping run
2025-06-16 23:03:24 | INFO     | base_scraper.py:139  | run_scraping() | fed_reserve: Last check: 2025-06-16 22:55:19.283048
2025-06-16 23:03:24 | INFO     | base_scraper.py:140  | run_scraping() | fed_reserve: Force update: True
2025-06-16 23:03:24 | INFO     | base_scraper.py:138  | run_scraping() | reddit_wsb: Starting scraping run
2025-06-16 23:03:24 | INFO     | base_scraper.py:139  | run_scraping() | reddit_wsb: Last check: 2025-06-16 23:03:17.107358
2025-06-16 23:03:24 | INFO     | base_scraper.py:140  | run_scraping() | reddit_wsb: Force update: True
2025-06-16 23:03:24 | INFO     | reddit_scraper.py:51   | scrape_new_content() | Reddit WSB: Looking for posts since 2025-06-16 22:33:17.107358
2025-06-16 23:03:24 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping hot posts
2025-06-16 23:03:26 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in hot
2025-06-16 23:03:27 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping new posts
2025-06-16 23:03:28 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in new
2025-06-16 23:03:29 | INFO     | reddit_scraper.py:58   | scrape_new_content() | Reddit WSB: Scraping rising posts
2025-06-16 23:03:30 | INFO     | reddit_scraper.py:61   | scrape_new_content() | Reddit WSB: Found 0 relevant posts in rising
2025-06-16 23:03:31 | DEBUG    | base_scraper.py:75   | _save_check_time() | reddit_wsb: Check time saved: 2025-06-16 23:03:24.207621
2025-06-16 23:03:31 | INFO     | base_scraper.py:186  | run_scraping() | reddit_wsb: Scraping completed successfully
2025-06-16 23:03:31 | INFO     | base_scraper.py:187  | run_scraping() | reddit_wsb: Found 0 items, saved 0 new
2025-06-16 23:03:31 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | reddit_wsb: ‚úÖ SUCCESS - 0 new items
2025-06-16 23:03:59 | ERROR    | base_scraper.py:192  | run_scraping() | fed_reserve: Scraping failed: Length mismatch: Expected axis has 0 elements, new values have 1 elements
Traceback (most recent call last):
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\base_scraper.py", line 144, in run_scraping
    new_content = self.scrape_new_content()
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\scrapers\fed_scraper.py", line 28, in scrape_new_content
    mpc_df = MonetaryPolicyCommittee(historical_split=2014, verbose=True).find_statements()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\FedTools\Beigebook.py", line 204, in find_beige_books
    self.dataset.columns = ['Beige_Book']
    ^^^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 6332, in __setattr__
    return object.__setattr__(self, name, value)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^
  File "pandas/_libs/properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\generic.py", line 814, in _set_axis
    self._mgr.set_axis(axis, labels)
    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\managers.py", line 238, in set_axis
    self._validate_set_axis(axis, new_labels)
    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^
  File "D:\YuanYuanGu\Documents\Projects\Trading\Screener\.screener\Lib\site-packages\pandas\core\internals\base.py", line 98, in _validate_set_axis
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Length mismatch: Expected axis has 0 elements, new values have 1 elements
2025-06-16 23:03:59 | INFO     | scraper_manager.py:96   | _run_scrapers_parallel() | fed_reserve: ‚ùå FAILED
2025-06-16 23:03:59 | INFO     | scraper_manager.py:58   | run_all_scrapers() | ============================================================
2025-06-16 23:03:59 | INFO     | scraper_manager.py:59   | run_all_scrapers() | MULTI-SCRAPER EXECUTION COMPLETE
2025-06-16 23:03:59 | INFO     | scraper_manager.py:60   | run_all_scrapers() | ============================================================
2025-06-16 23:03:59 | INFO     | scraper_manager.py:61   | run_all_scrapers() | Total execution time: 35792ms
2025-06-16 23:03:59 | INFO     | scraper_manager.py:62   | run_all_scrapers() | Successful scrapers: 1/2
2025-06-16 23:03:59 | INFO     | scraper_manager.py:63   | run_all_scrapers() | Total new content: 0
2025-06-16 23:03:59 | INFO     | test_scraper.py:304  | run_performance_test() | Performance comparison:
2025-06-16 23:03:59 | INFO     | test_scraper.py:305  | run_performance_test() |    Sequential: 33.31s
2025-06-16 23:03:59 | INFO     | test_scraper.py:306  | run_performance_test() |    Parallel: 35.79s
2025-06-16 23:03:59 | INFO     | test_scraper.py:312  | run_performance_test() |    ‚ö†Ô∏è  No significant performance improvement from parallel execution
2025-06-16 23:03:59 | INFO     | test_scraper.py:344  | main() |    ‚úÖ PASSED
2025-06-16 23:03:59 | INFO     | test_scraper.py:350  | main() | 
================================================================================
2025-06-16 23:03:59 | INFO     | test_scraper.py:351  | main() | SCRAPER SYSTEM TEST SUMMARY
2025-06-16 23:03:59 | INFO     | test_scraper.py:352  | main() | ================================================================================
2025-06-16 23:03:59 | INFO     | test_scraper.py:359  | main() | ‚ùå FAILED Fed Scraper
2025-06-16 23:03:59 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Reddit WSB Scraper
2025-06-16 23:03:59 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Scraper Manager
2025-06-16 23:03:59 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Change Detection
2025-06-16 23:03:59 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Database Cleanup
2025-06-16 23:03:59 | INFO     | test_scraper.py:359  | main() | ‚úÖ PASSED Performance Test
2025-06-16 23:03:59 | INFO     | test_scraper.py:361  | main() | --------------------------------------------------------------------------------
2025-06-16 23:03:59 | INFO     | test_scraper.py:362  | main() | OVERALL RESULT: 5/6 tests passed
2025-06-16 23:03:59 | ERROR    | test_scraper.py:368  | main() | ‚ö†Ô∏è  1 tests failed. Check the logs above for details.
